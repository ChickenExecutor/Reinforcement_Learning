{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44531cd0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7442806260debf35f53b95c2ecd776f6",
     "grade": false,
     "grade_id": "cell-4e7b1b0b06eb1947",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise 3: Off-Policy Methods\n",
    "\n",
    "---\n",
    "\n",
    "> **Reinforcement Learning** in Winter Semester 2025/2026\n",
    ">\n",
    "> - Prof. Gerhard Neumann and Prof. Rudolf Lioutikov\n",
    "> - Instructor for this exercise: Tai Hoang (tai.hoang@kit.edu)\n",
    "> \n",
    "> ⚠️ For general questions about the exercises, please post in the Ilias forum so we can answer them once for all students. Only use email for individual questions whose answers are not relevant to all students.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5277831f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f4e86756b92165c4f01842307fb039f0",
     "grade": false,
     "grade_id": "cell-b6cd880e81dc1595",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Group Submission\n",
    "\n",
    "The exercise sheets can be submitted in groups of up to **3 students**. **Only one member in the group is required to upload the final version via Ilias**.\n",
    "\n",
    "Groups are automatically recorded, so **please enter the u-identifiers of your group members in the following cell.** If your group consists of only 2 students, or if you are submitting alone, leave the remaining fields empty. Here is an **example** for a group consisting of uabcd and uefgh:\n",
    "\n",
    "_U-identifiers of group members:_\n",
    "\n",
    "_Member 1: uabcd_\n",
    "\n",
    "_Member 2: uefgh_\n",
    "\n",
    "_Member 3:_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790095cd",
   "metadata": {},
   "source": [
    "U-identifiers of group members:\n",
    "\n",
    "Member 1:\n",
    "\n",
    "Member 2:\n",
    "\n",
    "Member 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34484f75",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c54a009d44d1e0617d7715cbc3376808",
     "grade": false,
     "grade_id": "cell-109e42bfcc7f2efc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Auto-grading\n",
    "\n",
    "We use an auto-grading system that automatically analyzes your submitted Jupyter Notebooks and checks them for correctness using hidden tests. These tests determine the points you receive for the exercise sheet.\n",
    "\n",
    "To ensure auto-grading works smoothly, please note the following:\n",
    "\n",
    "- The notebook must have the filename \"ex_03_off_policy_methods.ipynb\"\n",
    "- Upload Jupyter Notebook on Ilias (not as a zip!)\n",
    "- Before submitting a notebook, test that everything runs without errors from start to finish.\n",
    "- Cells marked with \"##### DO NOT CHANGE #####\" must not be edited or deleted\n",
    "- Your solution must be entered in the correct cell (marked with \"# YOUR CODE HERE\").\n",
    "  - Please delete the **NotImplementedError!**\n",
    "- Generally, **do not delete any cells** and **do not add any cells**. The cells where your solution should be entered already exist (marked with \"# YOUR CODE HERE\").\n",
    "- There may be seemingly empty cells that are also marked with \"##### DO NOT CHANGE #####\". These must also not be edited or deleted.\n",
    "  - If you do modify them, auto-grading will not work and you will receive no points.\n",
    "  - We will be strict about this and make no exceptions if someone modifies cells clearly marked as readonly!\n",
    "- The Jupyter Notebooks have inline tests (visible to you) that check your result for basic correctness.\n",
    "  - These are primarily for you to identify and correct errors.\n",
    "  - However, the inline tests you can see in the notebook are not the tests used for grading!\n",
    "  - The inline tests are a necessary but not sufficient condition to receive points when grading the task!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c8021c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9ee39f86bc3555e998e058a02e81cc21",
     "grade": false,
     "grade_id": "cell-a5f13572f2ff72a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Local Setup\n",
    "\n",
    "If you prefer to work locally, see the following instructions for setting up Python in a virtual environment.\n",
    "You can then ignore the instructions in \"Colab Setup\".\n",
    "\n",
    "If you haven't yet, create a [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html) environment using:\n",
    "```\n",
    "conda create --name rl_exercises\n",
    "conda activate rl_exercises\n",
    "```\n",
    "Torch recommends installation using conda rather than pip, so run:\n",
    "```\n",
    "conda install pytorch cpuonly -c pytorch\n",
    "```\n",
    "If you have a CUDA-enabled GPU and would like to use it, visit [the installation page](https://pytorch.org/get-started/locally/) to see the options available for different CUDA versions.\n",
    "The remaining dependencies can be installed with pip:\n",
    "```\n",
    "pip install matplotlib numpy \"gymnasium[classic-control]\"\n",
    "```\n",
    "\n",
    "Even if you are running the Jupyter notebook locally, please run the code cells in **Colab Setup**, because they define some global variables required later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6f2281",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b731b5f25ff59b48d65ca86b7bf494fd",
     "grade": false,
     "grade_id": "cell-a1c56d3b3a0b2f96",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Colab Setup\n",
    "\n",
    "Google Colab provides you with a temporary environment for python programming.\n",
    "While this conveniently works on any platform and internally handles dependency issues and such, it also requires you to set up the environment from scratch every time.\n",
    "The \"Colab Setup\" section below will be part of **every** exercise and contains utility that is needed before getting started.\n",
    "\n",
    "There is a timeout of about ~12 hours with Colab while it is active (and less if you close your browser window).\n",
    "Any changes you make to the Jupyter notebook itself should be saved to your Google Drive.\n",
    "We also save all recordings and logs in it by default so that you won't lose your work in the event of an instance timeout.\n",
    "However, you will need to re-mount your Google Drive and re-install packages with every new instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfae86a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2308ea23802ea63793a7c12de974b3cc",
     "grade": false,
     "grade_id": "cell-0740323206b19df9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "\"\"\"Your work will be stored in a folder called `rl_ws25` by default to prevent Colab \n",
    "instance timeouts from deleting your edits.\n",
    "We do this by mounting your google drive on the virtual machine created in this colab \n",
    "session. For this, you will likely need to sign in to your Google account and allow\n",
    "access to your Google Drive files.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/gdrive\")\n",
    "    COLAB = True\n",
    "except ImportError:\n",
    "    COLAB = False\n",
    "\n",
    "# Create paths in your google drive\n",
    "if COLAB:\n",
    "    DATA_ROOT = Path(\"/content/gdrive/My Drive/rl_ws25\")\n",
    "    DATA_ROOT.mkdir(exist_ok=True)\n",
    "else:\n",
    "    DATA_ROOT = Path.cwd() / \"rl_ws25\"\n",
    "\n",
    "# Install **python** packages\n",
    "if COLAB:\n",
    "    %pip install matplotlib numpy \"gymnasium[classic-control]\" torch\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8a073b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d2dc67c06a07f7069b94da6fe51cb510",
     "grade": false,
     "grade_id": "cell-a308ccba99650463",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise 3: Soft Actor-Critic (SAC)\n",
    "\n",
    "In this exercise, we will implement the **Soft Actor-Critic (SAC)** algorithm. SAC is an off-policy actor-critic method that is widely used due to its sample efficiency and stability. It acts under a maximum entropy principle to ensure sufficient exploration during training.\n",
    "\n",
    "Key features of SAC:\n",
    "1. **Maximum entropy**: Encourages exploration by maximizing both reward and policy entropy\n",
    "2. **Twin Q-networks**: Uses two Q-networks to reduce overestimation bias\n",
    "3. **Polyak updates**: Soft target network updates for stability\n",
    "4. **Off-policy**: Learns from a replay buffer for sample efficiency\n",
    "\n",
    "We will test SAC on the [Pendulum-v1](https://gymnasium.farama.org/environments/classic_control/pendulum/) environment.\n",
    "\n",
    "We start by importing all the necessary python modules and defining some helper functions which you do not need to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3a2272",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca65213fbb530035dcba6bbb77d3633e",
     "grade": false,
     "grade_id": "cell-dc5d0a03b86b5d43",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import collections\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import gymnasium as gym\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# specify the path to save the recordings of this run to.\n",
    "DATA_PATH = DATA_ROOT / \"exercise_3\" / time.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "\n",
    "def save_figure(fig: plt.Figure, save_name: str) -> None:\n",
    "    \"\"\"Saves a figure into your google drive folder or local directory\"\"\"\n",
    "    DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    path = DATA_PATH / (save_name + \".png\")\n",
    "    # DO NOT CHANGE: Turn off save_figure\n",
    "    # fig.savefig(str(path))\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c62a44a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1204c842552f40c56ddb0a0b090c02c2",
     "grade": false,
     "grade_id": "cell-2fe2422b3d56b569",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Pendulum Environment\n",
    "\n",
    "The goal is to swing up a pendulum and keep it upright.\n",
    "\n",
    "- **State**: ($\\cos(\\theta)$, $\\sin(\\theta)$, $\\dot{\\theta}$) - 3D continuous\n",
    "- **Action**: torque in $[-2, 2]$ - 1D continuous  \n",
    "- **Reward**: Negative reward based on angle and velocity (higher is better, $\\max R \\approx 0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067c4f1d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a593142d1ab0fb3837be3f1958fccda",
     "grade": false,
     "grade_id": "cell-32406e7d46d8b066",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# Create environment\n",
    "env = gym.make('Pendulum-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_max = float(env.action_space.high[0])\n",
    "\n",
    "print(f\"State dimension: {state_dim}\")\n",
    "print(f\"Action dimension: {action_dim}\")\n",
    "print(f\"Action range: [{-action_max}, {action_max}]\")\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bb424a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "54a603a509361a8e8e85f4e16936a5f2",
     "grade": false,
     "grade_id": "cell-0cb8c726616d49c2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Replay Buffer\n",
    "\n",
    "Off-policy methods use a replay buffer to store and reuse past experiences. This breaks correlation between consecutive samples and improves sample efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674299b0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed712b36a000f5feaf7a7b8b17598b5a",
     "grade": false,
     "grade_id": "cell-67aee942aafb8432",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_limit: int, batch_size: int):\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "\n",
    "    def put(self, transition: tuple) -> None:\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self) -> tuple:\n",
    "        mini_batch = random.sample(self.buffer, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "\n",
    "        for transition in mini_batch:\n",
    "            state, action, reward, next_state, done = transition\n",
    "            states.append(state)\n",
    "            actions.append([action])\n",
    "            rewards.append([reward])\n",
    "            next_states.append(next_state)\n",
    "            dones.append([float(done)])\n",
    "\n",
    "        return torch.tensor(states, dtype=torch.float), \\\n",
    "               torch.tensor(actions, dtype=torch.float), \\\n",
    "               torch.tensor(rewards, dtype=torch.float), \\\n",
    "               torch.tensor(next_states, dtype=torch.float), \\\n",
    "               torch.tensor(dones, dtype=torch.float)\n",
    "\n",
    "    def size(self) -> int:\n",
    "        return len(self.buffer)\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b9866f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c3f6ba6c543e04b9e43d5e7eaed4e532",
     "grade": false,
     "grade_id": "cell-9adf2146db5c1911",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## **TASK 1: Actor/Policy network** (4 Points)\n",
    "\n",
    "Next, we will set up the actor/policy.\n",
    "\n",
    "### Task 1.1: Tanh Squashing\n",
    "The original SAC implementation squashes its actions into the $[-1, 1]$ range using a *tanh* activation.\n",
    "To ensure proper probabilities for these actions, it also squashes the log probabilities of each action accordingly.\n",
    "You can show using the change of variables theorem that\n",
    "\n",
    "\\begin{align}\n",
    "    \\log \\pi(\\boldsymbol{a}|\\boldsymbol{s}) = \\log \\mu(\\boldsymbol{u}|\\boldsymbol{s})-\\sum_{i=1}^D \\log\\left(1-\\tanh^2(u_i)\\right)\n",
    "\\end{align}\n",
    "\n",
    "for (squashed) actions $\\boldsymbol{a}$, states $\\boldsymbol{s}$, proposed (unsquashed) actions $\\boldsymbol{u}$ and a policy\n",
    "distribution $\\mu$ (which you shouldn't confuse with the mean, which is sometimes also called $\\mu$...)\n",
    "In our case, the dimensionality $D=1$.\n",
    "You will need to squash the action itself, as well as its log probability.\n",
    "\n",
    "### Task 1.2: Training from 2 Q-Networks\n",
    "As SAC is using Twin-Delayed Q-Networks to prevent the overestimation bias in the Q-Values,\n",
    "the actor/policy needs to choose the minimum of both available Q-Networks for its loss function.\n",
    "For this, you will need to evaluate the action using both Q-Networks, and then simply choose their minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50e84fc",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8da4fda372fca29ac88e2d70fbdb1440",
     "grade": false,
     "grade_id": "cell-2a7b70762628293f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, learning_rate: float, entropy_alpha: float):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(3, 128)\n",
    "        self.mean = nn.Linear(128, 1)\n",
    "        self.std = nn.Linear(128, 1)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.entropy_alpha = entropy_alpha\n",
    "    \n",
    "    def forward(self, state: torch.Tensor, deterministic: bool = False) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = F.relu(self.fc1(state))\n",
    "        mean = self.mean(x)\n",
    "        std = F.softplus(self.std(x))\n",
    "        \n",
    "        dist = Normal(mean, std)\n",
    "        \n",
    "        if deterministic:\n",
    "            action = mean\n",
    "        else:\n",
    "            action = dist.rsample()\n",
    "        \n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        # Hint: Apply tanh squashing and adjust log probabilities\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return real_action, real_log_prob\n",
    "    \n",
    "    def _compute_loss(self, q_net_1, q_net_2, states: torch.Tensor, actions: torch.Tensor, log_probs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Helper function to compute the policy loss.\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        entropy = -self.entropy_alpha * log_probs\n",
    "        loss = -(min_q + entropy).mean()\n",
    "        return loss, entropy, min_q\n",
    "    \n",
    "    def train_step(self, q_net_1, q_net_2, mini_batch: tuple) -> Dict[str, float]:\n",
    "        states, _, _, _, _ = mini_batch\n",
    "        actions, log_probs = self.forward(states)\n",
    "                \n",
    "        loss, entropy, _ = self._compute_loss(q_net_1, q_net_2, states, actions, log_probs)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()        \n",
    "        return {\"policy_loss\": loss.item(), \"entropy\": entropy.mean().item()}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0143c6eb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "354a3cd29017366a3080f64875e7b3d4",
     "grade": true,
     "grade_id": "cell-16be421b326b17db",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# ID: cell-16be421b326b17db - possible points: 4\n",
    "\n",
    "# Test PolicyNet\n",
    "print(\"Testing PolicyNet...\")\n",
    "test_policy = PolicyNet(learning_rate=3e-4, entropy_alpha=0.2)\n",
    "test_state = torch.randn(32, 3)\n",
    "test_action, test_log_prob = test_policy(test_state)\n",
    "\n",
    "assert test_action.shape == (32, 1), \"Action shape should be (batch_size, 1)\"\n",
    "assert test_log_prob.shape == (32, 1), \"Log prob shape should be (batch_size, 1)\"\n",
    "assert torch.all(test_action >= -1) and torch.all(test_action <= 1), \"Actions should be in [-1, 1]\"\n",
    "\n",
    "print(\"✓ PolicyNet passed basic tests!\")\n",
    "\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d36ad11",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1202df5325a55577fc4c2ce6026495c6",
     "grade": false,
     "grade_id": "cell-1ade0ce9c6d3e8e6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next, we implement our Q-Network. This will be used to evaluate (\"criticize\") the actions that the actor proposes.\n",
    "Note that this is only *one* Q-Network, and that the SAC class below will use multiple of those for the Twin-Delayed\n",
    "Q-Functions.\n",
    "\n",
    "## Task 2: Polyak Updates ( 2 Points)\n",
    "For increasing stability in the update of the Q Networks, SAC uses polyak updates of each Q Network.\n",
    "The update is given as\n",
    "\\begin{align}\n",
    "    \\beta'_i = (1-\\tau)\\beta'_i+(\\tau)\\beta_i\n",
    "\\end{align}\n",
    "\n",
    "for Q-Network parameters $\\beta_1$, $\\beta_2$ and an update rate $\\tau$. Note that the slides use a reverse\n",
    "order of $(1-\\tau)$ and $\\tau$, which corresponds to values of $\\tau$ close to $1$ rather than close to $0$\n",
    "as done in the code.\n",
    "\n",
    "Hint: Use the `parameters().data` attribute of torch.Tensor to access the parameters. You can copy them\n",
    "using `parameters().data.copy_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5479e0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "817849e181022964bd498ea6d1320484",
     "grade": false,
     "grade_id": "cell-76d267a6864f68dd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class QNet(nn.Module):\n",
    "    def __init__(self, learning_rate: float, tau: float):\n",
    "        super(QNet, self).__init__()\n",
    "        \n",
    "        self.state_fc = nn.Linear(3, 64)\n",
    "        self.action_fc = nn.Linear(1, 64)\n",
    "        self.fc1 = nn.Linear(128, 32)\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.tau = tau\n",
    "    \n",
    "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
    "        h1 = F.relu(self.state_fc(state))\n",
    "        h2 = F.relu(self.action_fc(action))\n",
    "        x = torch.cat([h1, h2], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "    \n",
    "    def train_step(self, target_values: torch.Tensor, mini_batch: tuple) -> float:\n",
    "        states, actions, _, _, _ = mini_batch\n",
    "        q_values = self.forward(states, actions)\n",
    "        \n",
    "        loss = F.smooth_l1_loss(q_values, target_values)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def polyak_update(self, target_network):\n",
    "        \"\"\"Soft update of target network parameters\"\"\"\n",
    "        # Implement Polyak update\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb624def",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "57acaaadd2f0e08a577360bc0813d5da",
     "grade": true,
     "grade_id": "cell-8b5a9eed4dc8caa6",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# ID: cell-8b5a9eed4dc8caa6 - possible points: 2\n",
    "\n",
    "# Test QNet and Polyak update\n",
    "print(\"Testing QNet...\")\n",
    "test_q = QNet(learning_rate=3e-4, tau=0.01)\n",
    "test_q_target = copy.deepcopy(test_q)\n",
    "\n",
    "test_state = torch.randn(32, 3)\n",
    "test_action = torch.randn(32, 1)\n",
    "test_q_value = test_q(test_state, test_action)\n",
    "\n",
    "assert test_q_value.shape == (32, 1), \"Q-value shape should be (batch_size, 1)\"\n",
    "\n",
    "# Test Polyak update\n",
    "initial_params = [p.clone() for p in test_q_target.parameters()]\n",
    "test_q.polyak_update(test_q_target)\n",
    "final_params = [p for p in test_q_target.parameters()]\n",
    "\n",
    "changes_list = []\n",
    "for init_p, final_p in zip(initial_params, final_params):\n",
    "    changes_list.append((final_p - init_p).abs().mean().item())\n",
    "    \n",
    "assert any(change > 0 for change in changes_list), \"Target network should change after Polyak update\"\n",
    "print(\"✓ QNet passed basic tests!\")\n",
    "\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ab0160",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b61b8fa9a19af6e67becc27ceefca74b",
     "grade": false,
     "grade_id": "cell-4318d4182a5681d7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 3: SAC with Target Q-Value Computation (6 Points)\n",
    "\n",
    "SAC computes target Q-values using:\n",
    "\n",
    "$$y = r + \\gamma (1 - \\text{done}) \\cdot \\left(\\min(Q_{\\theta_1'}(s', a'), Q_{\\theta_2'}(s', a')) - \\alpha \\log \\pi(a'|s')\\right)$$\n",
    "\n",
    "where $a' \\sim \\pi(\\cdot|s')$ is sampled from the policy.\n",
    "\n",
    "**Your task**: Implement the `calculate_q_targets` method.\n",
    "\n",
    "**Key points**:\n",
    "- Use the **target** Q-networks (not the main ones)\n",
    "- Take the **minimum** of both target Q-values\n",
    "- Include the entropy bonus term\n",
    "- Don't update if episode is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0597ce",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f2de722b4f9ee4512bdb8b1b8b3b620",
     "grade": false,
     "grade_id": "cell-7d6e3750ec1c030f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class SoftActorCritic:\n",
    "    def __init__(self, learning_rate=3e-4, tau=0.005, gamma=0.98, entropy_alpha=0.2, \n",
    "                 batch_size=64, buffer_limit=100000):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Initialize networks\n",
    "        self.policy = PolicyNet(learning_rate, entropy_alpha)\n",
    "        \n",
    "        self.q_net_1 = QNet(learning_rate, tau)\n",
    "        self.q_net_2 = QNet(learning_rate, tau)\n",
    "        \n",
    "        self.q_net_1_target = copy.deepcopy(self.q_net_1)\n",
    "        self.q_net_2_target = copy.deepcopy(self.q_net_2)\n",
    "        \n",
    "        self.memory = ReplayBuffer(buffer_limit, batch_size)\n",
    "    \n",
    "    def calculate_q_targets(self, mini_batch: tuple) -> torch.Tensor:\n",
    "        \"\"\"Calculate target Q-values for training\"\"\"\n",
    "        _, _, rewards, next_states, dones = mini_batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Calculate target Q-values\n",
    "            # Hint:\n",
    "            # 1. Sample next action from policy: next_action, log_prob = self.policy(next_states)\n",
    "            # 2. Get Q-values from both target networks\n",
    "            # 3. Take minimum of both Q-values\n",
    "            # 4. Calculate entropy bonus\n",
    "            # 5. Calculate target\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "        \n",
    "        return target\n",
    "    \n",
    "    def train_step(self) -> Dict[str, float]:\n",
    "        mini_batch = self.memory.sample()\n",
    "        \n",
    "        # Calculate targets\n",
    "        q_targets = self.calculate_q_targets(mini_batch)\n",
    "        \n",
    "        # Update Q-networks\n",
    "        q1_loss = self.q_net_1.train_step(q_targets, mini_batch)\n",
    "        q2_loss = self.q_net_2.train_step(q_targets, mini_batch)\n",
    "        \n",
    "        # Update policy\n",
    "        policy_metrics = self.policy.train_step(self.q_net_1, self.q_net_2, mini_batch)\n",
    "        \n",
    "        # Polyak updates\n",
    "        self.q_net_1.polyak_update(self.q_net_1_target)\n",
    "        self.q_net_2.polyak_update(self.q_net_2_target)\n",
    "        \n",
    "        return {\"q1_loss\": q1_loss, \"q2_loss\": q2_loss, **policy_metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443452a5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b3e60c82d8b47bf92a73b37d5e52cb50",
     "grade": true,
     "grade_id": "cell-bcc8726c870cbe4e",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# ID: cell-bcc8726c870cbe4e - possible points: 6\n",
    "\n",
    "# Test SAC\n",
    "print(\"Testing SAC...\")\n",
    "test_sac = SoftActorCritic()\n",
    "\n",
    "# Fill replay buffer\n",
    "for _ in range(1000):\n",
    "    s = env.observation_space.sample()\n",
    "    a = env.action_space.sample()[0]\n",
    "    r = np.random.randn()\n",
    "    ns = env.observation_space.sample()\n",
    "    d = np.random.rand() > 0.9\n",
    "    test_sac.memory.put((s, a, r, ns, d))\n",
    "\n",
    "# Test training step\n",
    "metrics = test_sac.train_step()\n",
    "assert 'q1_loss' in metrics, \"Should return q1_loss\"\n",
    "assert 'q2_loss' in metrics, \"Should return q2_loss\"\n",
    "assert 'policy_loss' in metrics, \"Should return policy_loss\"\n",
    "\n",
    "print(\"✓ SAC passed basic tests!\")\n",
    "\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7037c25",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6114331e499ee70dccacca587ae77f31",
     "grade": false,
     "grade_id": "cell-caa8076c18a5fe5e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Training SAC\n",
    "\n",
    "Now let's train SAC on the Pendulum environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d3e7ba",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f454a77ac0b67bf8b8ff6c3967c3f4fd",
     "grade": false,
     "grade_id": "cell-54b2bd1fcc669e1e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "def evaluate_policy(sac, n_episodes=5):\n",
    "    \"\"\"Evaluate policy without exploration\"\"\"\n",
    "    eval_env = gym.make('Pendulum-v1')\n",
    "    scores = []\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        state, _ = eval_env.reset(seed=SEED)\n",
    "        score = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                action, _ = sac.policy(torch.FloatTensor(state).unsqueeze(0), deterministic=True)\n",
    "            scaled_action = action_max * action.item()\n",
    "            state, reward, terminated, truncated, _ = eval_env.step([scaled_action])\n",
    "            done = terminated or truncated\n",
    "            score += reward\n",
    "        \n",
    "        scores.append(score)\n",
    "    \n",
    "    eval_env.close()\n",
    "    return np.mean(scores)\n",
    "\n",
    "# Training loop\n",
    "sac = SoftActorCritic()\n",
    "\n",
    "num_steps = 30000\n",
    "eval_freq = 1000\n",
    "reward_scale = 0.1\n",
    "\n",
    "state, _ = env.reset(seed=SEED)\n",
    "scores = []\n",
    "steps = []\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Select action\n",
    "    with torch.no_grad():\n",
    "        action, _ = sac.policy(torch.FloatTensor(state).unsqueeze(0))\n",
    "    scaled_action = action_max * action.item()\n",
    "    \n",
    "    # Environment step\n",
    "    next_state, reward, terminated, truncated, _ = env.step([scaled_action])\n",
    "    done = terminated or truncated\n",
    "    \n",
    "    # Store transition\n",
    "    sac.memory.put((state, action.item(), reward * reward_scale, next_state, done))\n",
    "    \n",
    "    state = env.reset(seed=SEED)[0] if done else next_state\n",
    "    \n",
    "    # Train\n",
    "    if sac.memory.size() > 1000:\n",
    "        sac.train_step()\n",
    "    \n",
    "    # Evaluate\n",
    "    if step % eval_freq == 0:\n",
    "        score = evaluate_policy(sac)\n",
    "        scores.append(score)\n",
    "        steps.append(step)\n",
    "        print(f\"Step {step}, Score: {score:.2f}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Plot results\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(steps, scores)\n",
    "ax.set_xlabel('Training Steps')\n",
    "ax.set_ylabel('Average Return')\n",
    "ax.set_title('SAC Training on Pendulum-v1')\n",
    "ax.grid(True, alpha=0.3)\n",
    "save_figure(fig, \"sac_training\")\n",
    "plt.show()\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d461115",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "da149dbbf611b5cdd7570be6f7d7a304",
     "grade": false,
     "grade_id": "cell-ea9d6e991cff8a63",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Analysis\n",
    "\n",
    "You should see the performance improve over time. SAC typically achieves near-optimal performance (scores around -150 to -200) after 20,000-30,000 steps.\n",
    "\n",
    "Key observations:\n",
    "- The entropy bonus encourages exploration early in training\n",
    "- Twin Q-networks reduce overestimation bias\n",
    "- Polyak updates provide stable learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a692cd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7eff5cf3fa08a870bfec17883d5aa258",
     "grade": false,
     "grade_id": "cell-e0b06fcaa12828ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Visualization\n",
    "\n",
    "Let's visualize the learned value function by evaluating Q-values across the state space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1ee4b4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "91bc6235e7bb15a1ec8ec14a656f1af7",
     "grade": false,
     "grade_id": "cell-4219a2d2692686b8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "def plot_value_function(sac, resolution=100):\n",
    "    \"\"\"Plot the value function by evaluating Q-values across the state space\"\"\"\n",
    "    # Create grid of states\n",
    "    max_speed = 8\n",
    "    theta = np.linspace(-np.pi, np.pi, resolution)\n",
    "    theta_dot = np.linspace(-max_speed, max_speed, resolution)\n",
    "    \n",
    "    # Create meshgrid for all state combinations\n",
    "    theta_grid, theta_dot_grid = np.meshgrid(theta, theta_dot)\n",
    "    \n",
    "    # Convert to observations: (cos(theta), sin(theta), theta_dot)\n",
    "    cos_theta = np.cos(theta_grid.flatten())\n",
    "    sin_theta = np.sin(theta_grid.flatten())\n",
    "    theta_dot_flat = theta_dot_grid.flatten()\n",
    "    \n",
    "    states = torch.tensor(np.stack([cos_theta, sin_theta, theta_dot_flat], axis=1), dtype=torch.float)\n",
    "    \n",
    "    # Evaluate value function by taking max over actions\n",
    "    action_samples = np.linspace(-1, 1, 50)  # Sample actions in [-1, 1] range\n",
    "    max_q_values = np.full(len(states), -np.inf)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for action_val in action_samples:\n",
    "            actions = torch.full((len(states), 1), action_val, dtype=torch.float)\n",
    "            q1 = sac.q_net_1(states, actions)\n",
    "            q2 = sac.q_net_2(states, actions)\n",
    "            min_q = torch.min(q1, q2).squeeze().numpy()\n",
    "            max_q_values = np.maximum(max_q_values, min_q)\n",
    "    \n",
    "    # Reshape for plotting\n",
    "    v_values = max_q_values.reshape(resolution, resolution)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    contour = ax.contourf(theta, theta_dot, v_values, levels=100, cmap='viridis')\n",
    "    plt.colorbar(contour, ax=ax, label='Value')\n",
    "    ax.set_xlabel(r'$\\theta$ (angle)')\n",
    "    ax.set_ylabel(r'$\\dot{\\theta}$ (angular velocity)')\n",
    "    ax.set_title('Learned Value Function V(s)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    save_figure(fig, \"value_function\")\n",
    "    plt.show()\n",
    "\n",
    "plot_value_function(sac)\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195cea26",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d28cb754c2609ded383f0fd717852969",
     "grade": false,
     "grade_id": "cell-a54c0295124906ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's watch how the agent perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74845c3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dcbbb79b2908d6cd9db4651fbbdcff7b",
     "grade": false,
     "grade_id": "cell-51d9740cd2fa1cdc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "def visualize_episode(sac, max_steps=200):\n",
    "    \"\"\"Visualize the agent performing in the environment\"\"\"\n",
    "    eval_env = gym.make('Pendulum-v1', render_mode='rgb_array')\n",
    "    \n",
    "    state, _ = eval_env.reset(seed=SEED)\n",
    "    frames = []\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Render current frame\n",
    "        frame = eval_env.render()\n",
    "        frames.append(frame)\n",
    "        \n",
    "        # Get action from policy\n",
    "        with torch.no_grad():\n",
    "            action, _ = sac.policy(torch.FloatTensor(state).unsqueeze(0), deterministic=True)\n",
    "        scaled_action = action_max * action.item()\n",
    "        \n",
    "        # Step environment\n",
    "        state, reward, terminated, truncated, _ = eval_env.step([scaled_action])\n",
    "        total_reward += reward\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    eval_env.close()\n",
    "    \n",
    "    # Create animation\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.axis('off')\n",
    "    img = ax.imshow(frames[0])\n",
    "    \n",
    "    def update(frame_idx):\n",
    "        img.set_data(frames[frame_idx])\n",
    "        ax.set_title(f'Step {frame_idx}/{len(frames)} | Total Reward: {total_reward:.1f}')\n",
    "        return [img]\n",
    "    \n",
    "    from matplotlib.animation import FuncAnimation\n",
    "    from IPython.display import HTML\n",
    "    \n",
    "    anim = FuncAnimation(fig, update, frames=len(frames), interval=50, blit=True)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Episode completed in {len(frames)} steps with total reward: {total_reward:.2f}\")\n",
    "    return HTML(anim.to_jshtml())\n",
    "\n",
    "# Visualize the trained policy\n",
    "visualize_episode(sac)\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
