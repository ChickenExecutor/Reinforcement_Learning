{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8352dd9a2f1178dd19c83629f8ca0749",
     "grade": false,
     "grade_id": "cell-a1936174b26b80c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Reinforcement Learning - Winter Semester 2025/26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e647544ece628919a00771292d20a2a",
     "grade": false,
     "grade_id": "cell-f0af853dff9f59b8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise 1: Tabular Reinforcement Learning\n",
    "\n",
    "---\n",
    "\n",
    "> **Reinforcement Learning** in Winter Semester 2025/2026\n",
    ">\n",
    "> - Prof. Gerhard Neumann and Prof. Rudolf Lioutikov\n",
    "> - Instructor for this exercise: Tai Hoang (tai.hoang@kit.edu)\n",
    "> \n",
    "> ⚠️ For general questions about the exercises, please post in the Ilias forum so we can answer them once for all students. Only use email for individual questions whose answers are not relevant to all students.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0f2853bd23e26d6743e84621ed0014c",
     "grade": false,
     "grade_id": "cell-e02f69a4d7d70474",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Group Submission\n",
    "\n",
    "The exercise sheets can be submitted in groups of up to **3 students**. **Each person in the group must upload the final version via Ilias**. It is not sufficient if only one person from the group does this. It is possible to join a new group during the semester if your own group dissolves early. Each group must upload their own solution, and we will check submissions for duplicates.\n",
    "\n",
    "Groups are automatically recorded, so **please enter the u-identifiers of your group members in the following cell.** If your group consists of only 2 students, or if you are submitting alone, leave the remaining fields empty. Here is an **example** for a group consisting of uabcd and uefgh:\n",
    "\n",
    "_U-identifiers of group members:_\n",
    "\n",
    "_Member 1: uabcd_\n",
    "\n",
    "_Member 2: uefgh_\n",
    "\n",
    "_Member 3:_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-identifiers of group members:\n",
    "\n",
    "Member 1:\n",
    "\n",
    "Member 2:\n",
    "\n",
    "Member 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6779c5ec119673f314647c457c9f9be1",
     "grade": false,
     "grade_id": "cell-8605bee6d78c8771",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Auto-grading\n",
    "\n",
    "We use an auto-grading system that automatically analyzes your submitted Jupyter Notebooks and checks them for correctness using hidden tests. These tests determine the points you receive for the exercise sheet.\n",
    "\n",
    "To ensure auto-grading works smoothly, please note the following:\n",
    "\n",
    "- The notebook must have the filename \"ex_01_tabular_rl.ipynb\"\n",
    "- Upload PDF and Jupyter Notebook separately on Ilias (not as a zip!)\n",
    "- Before submitting a notebook, test that everything runs without errors from start to finish.\n",
    "- Cells marked with \"##### DO NOT CHANGE #####\" must not be edited or deleted\n",
    "- Your solution must be entered in the correct cell (marked with \"# YOUR CODE HERE\").\n",
    "  - Please delete the **NotImplementedError!**\n",
    "- Generally, **do not delete any cells** and **do not add any cells**. The cells where your solution should be entered already exist (marked with \"# YOUR CODE HERE\").\n",
    "- There may be seemingly empty cells that are also marked with \"##### DO NOT CHANGE #####\". These must also not be edited or deleted.\n",
    "  - If you do modify them, auto-grading will not work and you will receive no points.\n",
    "  - We will be strict about this and make no exceptions if someone modifies cells clearly marked as readonly!\n",
    "- The Jupyter Notebooks have inline tests (visible to you) that check your result for basic correctness.\n",
    "  - These are primarily for you to identify and correct errors.\n",
    "  - However, the inline tests you can see in the notebook are not the tests used for grading!\n",
    "  - The inline tests are a necessary but not sufficient condition to receive points when grading the task!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d1f2fe6fb36355b32accfc7e693ad38",
     "grade": false,
     "grade_id": "cell-f1d2b3139a4bc808",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Local Setup\n",
    "\n",
    "If you prefer to work locally, see the following instructions for setting up Python in a virtual environment.\n",
    "You can then ignore the instructions in \"Colab Setup\".\n",
    "\n",
    "If you haven't yet, create a [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html) environment using:\n",
    "```\n",
    "conda create --name rl_exercises\n",
    "conda activate rl_exercises\n",
    "```\n",
    "The remaining dependencies can be installed with pip:\n",
    "```\n",
    "pip install matplotlib pillow numpy ipykernel \"gymnasium[toy-text]\"\n",
    "```\n",
    "\n",
    "Even if you are running the Jupyter notebook locally, please run the code cell in **Colab Setup**, because it defines some global variables required later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b8f148e6a946c38fbba0044042b09397",
     "grade": false,
     "grade_id": "cell-595ee04fc84d0ab2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Colab Setup\n",
    "\n",
    "Google Colab provides you with a temporary environment for python programming.\n",
    "While this conveniently works on any platform and internally handles dependency issues and such, it also requires you to set up the environment from scratch every time.\n",
    "The \"Colab Setup\" section below will be part of **every** exercise and contains utility that is needed before getting started.\n",
    "\n",
    "There is a timeout of about ~12 hours with Colab while it is active (and less if you close your browser window).\n",
    "Any changes you make to the Jupyter notebook itself should be saved to your Google Drive.\n",
    "We also save all recordings and logs in it by default so that you won't lose your work in the event of an instance timeout.\n",
    "However, you will need to re-mount your Google Drive and re-install packages with every new instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f3f31661e60f962b48b6fb6e096f1e9",
     "grade": false,
     "grade_id": "cell-d35f108fcb4c3f36",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "\"\"\"Your work will be stored in a folder called `rl_ws25` by default to prevent Colab \n",
    "instance timeouts from deleting your edits.\n",
    "We do this by mounting your google drive on the virtual machine created in this colab \n",
    "session. For this, you will likely need to sign in to your Google account and allow\n",
    "access to your Google Drive files.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/gdrive\")\n",
    "    COLAB = True\n",
    "except ImportError:\n",
    "    COLAB = False\n",
    "\n",
    "# Create paths in your google drive\n",
    "if COLAB:\n",
    "    DATA_ROOT = Path(\"/content/gdrive/My Drive/rl_ws25\")\n",
    "    DATA_ROOT.mkdir(exist_ok=True)\n",
    "else:\n",
    "    DATA_ROOT = Path.cwd() / \"rl_ws25\"\n",
    "\n",
    "# Install **python** packages\n",
    "if COLAB:\n",
    "    %pip install matplotlib pillow numpy \"gymnasium[toy-text]\"\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f497d481b460d65beabf443a79b5c8e1",
     "grade": false,
     "grade_id": "cell-60ea2df03c2a4ded",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise 1: Tabular Reinforcement Learning\n",
    "\n",
    "In this homework, we will implement basic planning and reinforcement learning algorithms.\n",
    "We will look at Policy Iteration and Value Iteration, as well as tabular Q-Learning.\n",
    "The algorithms will be evaluated on a gridworld task from OpenAI gym.\n",
    "\n",
    "All homeworks are self-contained.\n",
    "They can be completed in their respective notebooks.\n",
    "Please fill in any missing code or answer any questions that are marked with `# YOUR CODE HERE` comments.\n",
    "Questions marked as \"Self-Test Questions\" are optional and do **not** need to be answered for points - they are for your own learning.\n",
    "To edit and re-run code, you can simply edit and restart the code cells below.\n",
    "\n",
    "***IMPORTANT!***\n",
    "\n",
    "**Submission:** When you are finished, please upload:\n",
    "1. The completed Jupyter Notebook (`ex_01_tabular_rl.ipynb` file)\n",
    "\n",
    "on Ilias (not as a zip).\n",
    "\n",
    "We start by importing all the necessary python modules and defining some helper functions which you do not need to change.\n",
    "Still, make sure you are aware of what they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e0bb17d1b0d5845cf82cc24f8fc5a21",
     "grade": false,
     "grade_id": "cell-30871b071c03797e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "# specify the path to save the recordings of this run to.\n",
    "DATA_PATH = DATA_ROOT / \"exercise_1\" / time.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "\n",
    "def save_figure(fig: plt.Figure, save_name: str) -> None:\n",
    "    \"\"\"Saves a figure into your google drive folder or local directory\"\"\"\n",
    "    DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    path = DATA_PATH / (save_name + \".png\")\n",
    "\n",
    "    # DO NOT CHANGE: Turn off save_figure\n",
    "    # fig.savefig(str(path))\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aecbfad3a6c4d6c23b9145aa055bd3b3",
     "grade": false,
     "grade_id": "cell-cf51b8f3134b8f5e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "import gymnasium as gym  # gymnasium is successor to gym\n",
    "from gymnasium.envs.toy_text.frozen_lake import FrozenLakeEnv\n",
    "\n",
    "\n",
    "def measure_policy_success(\n",
    "    env: FrozenLakeEnv,\n",
    "    pi: np.ndarray,\n",
    "    n_eval: int,\n",
    "    render: bool = False,\n",
    "    fps: float | None = None,\n",
    ") -> float:\n",
    "    \"\"\"Evaluate a policy on an environment and return success rate\n",
    "\n",
    "    :param env: FrozenLake env\n",
    "    :param pi: a policy that dictates the deterministic action to take in each state [n_states]\n",
    "    :param render: render the trajectory?\n",
    "    :return: The mean success rate of the given policy\n",
    "    \"\"\"\n",
    "\n",
    "    successes = []\n",
    "    for _ in range(n_eval):\n",
    "        state, info = env.reset()\n",
    "        if render:\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(Image.fromarray(env.render()))\n",
    "            if fps is not None:\n",
    "                time.sleep(1 / fps)\n",
    "        for _ in range(100):\n",
    "            action = int(pi[state])\n",
    "\n",
    "            state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            if render:\n",
    "                display.clear_output(wait=True)\n",
    "                display.display(Image.fromarray(env.render()))\n",
    "                if fps is not None:\n",
    "                    time.sleep(1 / fps)\n",
    "\n",
    "            if terminated or truncated:\n",
    "                successes.append(reward)\n",
    "                break\n",
    "\n",
    "    return np.mean(successes)\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9ed06819ef503c75405332a3cf22b1ae",
     "grade": false,
     "grade_id": "cell-9f19660437087b10",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## The FrozenLake Environment\n",
    "First, let's have a look at the problem we are solving. The agent controls the movement of a character in a grid world.\n",
    "Some tiles of the grid are walkable, and others lead to the agent falling into the water.\n",
    "Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction.\n",
    "The agent is rewarded for finding a walkable path to a goal tile.\n",
    "The episode ends when you reach the goal or fall in a hole.\n",
    "You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "Actions are encoded as integers 0 = left, 1 = down, 2 = right and 3 = up.\n",
    "The states are counted from 0 to $N_{states}$.\n",
    "\n",
    "![The FrozenLake Environment](https://gymnasium.farama.org/_images/frozen_lake.gif)\n",
    "\n",
    "See https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n",
    "\n",
    "### Hint\n",
    "Like all DiscreteEnvs in Gym, FrozenLake has a property `P` which is a dictionary of lists, where\n",
    "```\n",
    "P[s][a] == [(transition probability, next state, reward, done), ...]\n",
    "```\n",
    "for a state $s$ and an action $a$.\n",
    "In FrozenLake, the player will move in intended direction with probability of $1/3$ else will move in either perpendicular direction with equal probability of $1/3$ in both directions.\n",
    "In non-terminal states, `len(P[s][a]) == 3`.\n",
    "This environment is very similar to the one shown in the Optimal Decision Making lecture.\n",
    "\n",
    "Execute the next cell to see what a randomly initialized policy does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "90803e167779e2c66f4de48544cfb622",
     "grade": false,
     "grade_id": "cell-51b0cf156485ccb2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "env: FrozenLakeEnv = gym.make(\"FrozenLake-v1\", map_name=\"8x8\", render_mode=\"rgb_array\")\n",
    "env.reset(seed=SEED)\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.n\n",
    "pi_random = np.random.default_rng(SEED).integers(n_actions, size=(n_states,))\n",
    "\n",
    "success_rate = measure_policy_success(env, pi_random, n_eval=3, render=True, fps=30 if not COLAB else None)\n",
    "\n",
    "print(f\"Random policy success rate={success_rate}\")\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "66e6cb410bad1f1e14cf82003b28976a",
     "grade": false,
     "grade_id": "cell-1cf269ddfbfb6e1b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 1: Policy Iteration\n",
    "The first algorithm we will implement is called Policy Iteration.\n",
    "It consists of an inner and an outer loop.\n",
    "The inner loop is called Policy Evaluation and computes state values for the _current policy_.\n",
    "For every state, it averages the expected returns of all possible actions in that state, weighted by the current policy's action probabilities.\n",
    "The outer loop is called Policy Improvement and it uses the current policy's value function to return an improved policy.\n",
    "It does this by choosing an action for each state that maximizes the expected value of the next state.\n",
    "In Policy Iteration, we first initialize a policy and value function randomly, and then iteratively run Policy Evaluation and Policy Improvement until convergence.\n",
    "\n",
    "The **pseudocode** looks as follows:\n",
    "\n",
    "---\n",
    "- **Initialize** $V_{(0)}^{\\pi_0}(s)$ randomly for all $s$, $\\pi_0 \\leftarrow$ uniform, $k = 0$\n",
    "\n",
    "- **Repeat** for $i=1, 2, \\dots$\n",
    "\n",
    "    - **Policy Evaluation:**\n",
    "\n",
    "        - **Initialize** $V_{(0)}^{\\pi_{i}}(s) \\leftarrow V_{(k)}^{\\pi_{i-1}}(s)$ (i.e. initialize the value function of the new policy with the converged value function of the old policy)\n",
    "\n",
    "        - **Repeat** for $k=1, 2, \\dots$\n",
    "\n",
    "            \\begin{equation*}\n",
    "                V_{(k)}^{\\pi_{i}}(s) \\leftarrow \\sum_a \\pi(a \\mid s) \\sum_{s'} P(s' \\mid s,a) \\Big( r(s,a,s') + \\overline{d} \\gamma \\, V_{(k - 1)}^{\\pi_{i}}(s') \\Big)\n",
    "            \\end{equation*}\n",
    "\n",
    "        - **Until convergence**\n",
    "\n",
    "    - **Policy Improvement:**\n",
    "\n",
    "        \\begin{align*}\n",
    "            \\pi_{i}(a \\mid s) \\leftarrow \\begin{cases}\n",
    "                1, & \\text{if } a = \\underset{a'}{\\arg \\max} \\sum_{s'} P(s' \\mid s,a') \\Big( r(s,a',s') + \\overline{d} \\gamma \\, V^{\\pi_i}(s') \\Big)\\\\\n",
    "                0 & \\text{else}\n",
    "            \\end{cases}\n",
    "        \\end{align*}\n",
    "\n",
    "- **Until convergence**\n",
    "---\n",
    "\n",
    "Because the reward depends on the next state and not just the action (i.e. $r=r(s,a,s')$), we must slightly update the equations for policy iteration and value iteration seen in the lecture.\n",
    "\n",
    "Also note the addition of $\\overline{d}$ to the formulas from the lecture.\n",
    "The expected value of some transition is the transition reward plus the (discounted) value of the next state.\n",
    "However, some transitions result in termination (e.g. falling through the ice).\n",
    "For these transitions, the trajectory is over and we cannot use the (discounted) value of the next state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "490cbf01c8d5c860deeda8032ee8ac9f",
     "grade": false,
     "grade_id": "cell-3aa1bf8345655ee3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 1.1 (4 Points)\n",
    "\n",
    "In this first task you have to implement the core algorithm of **policy evaluation** and **policy improvement**, both marked with `# YOUR CODE HERE`.\n",
    "For convenience in this discrete environment, we maintain both an array called `pi_prob` for the _action probabilities_ and an array called `pi` that just contains the deterministic action that the policy takes in each state.\n",
    "\n",
    "After you have finished implementing the corresponding functions, execute the code cell.\n",
    "Your algorithm should be able to solve the task with around ~65% success rate.\n",
    "\n",
    "Note: the dynamics of the environment are available to you through the `mdp` variable.\n",
    "If you index this with the state and the action (i.e. `mdp[state][action]`), you are given a list of possible transitions, where each transition is a tuple of the form `(transition_prob: float, next_state: int, reward: float, done: bool)`.\n",
    "In other words, each transition has an associated probability, reward, and next state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dba0e2c18bba13bc4a4bb4fd04c403ed",
     "grade": false,
     "grade_id": "exercise_1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def policy_evaluation(\n",
    "    mdp: dict[int, dict[int, list[tuple[float, int, float, bool]]]],\n",
    "    pi_prob: np.ndarray,\n",
    "    previous_value: np.ndarray,\n",
    "    gamma: float,\n",
    "    max_iters: int = 10000,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Perform the Policy Evaluation step given a policy pi and an environment.\n",
    "\n",
    "    :param mdp: an MDP as a list of transitions for each state-action pair\n",
    "    :param pi_prob: Action probabilities [num_states, num_actions]\n",
    "    :param previous_value: Initial value function [num_states]\n",
    "    :return: value function of the provided policy [num_states]\n",
    "    \"\"\"\n",
    "    n_states, n_actions = pi_prob.shape\n",
    "    for _ in range(max_iters):\n",
    "        value = np.zeros_like(previous_value)\n",
    "        # hint: you will need to iterate over states and actions here\n",
    "        # get a list of possible transitions using mdp[state][action], which returns a list of\n",
    "        # tuples of (transition_prob, next_state, reward, done)\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # run policy evaluation until convergence\n",
    "        if np.allclose(value, previous_value):\n",
    "            break\n",
    "\n",
    "        # save current value estimate for next iteration\n",
    "        previous_value = np.copy(value)\n",
    "\n",
    "    return value\n",
    "\n",
    "def policy_improvement(\n",
    "    mdp: dict[int, dict[int, list[tuple[float, int, float, bool]]]],\n",
    "    value: np.ndarray,\n",
    "    gamma: float,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Perform the Policy Improvement step given a value function.\n",
    "\n",
    "    :param mdp: an MDP as a list of transitions for each state-action pair\n",
    "    :param value: Value function of a policy [num_states]\n",
    "    :return: New policy [num_states] and distribution over action probabilities [num_states, num_actions]\n",
    "    \"\"\"\n",
    "    n_states, n_actions = len(mdp), len(mdp[0])\n",
    "    # initialize policy\n",
    "    # contains the actual actions\n",
    "    pi = np.zeros(shape=n_states, dtype=np.int64)\n",
    "    # contains the action probabilities for each state\n",
    "    pi_prob = np.zeros(shape=(n_states, n_actions))\n",
    "\n",
    "    # hint: This again requires you to iterate over states and actions in some way.\n",
    "    # get a list of possible transitions using mdp[state][action], which returns a list of\n",
    "    # tuples of (transition_prob, next_state, reward, done)\n",
    "    # You can use np.argmax() to get the index of the biggest value in an array.\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return pi, pi_prob\n",
    "\n",
    "def policy_iteration(\n",
    "    env: FrozenLakeEnv,\n",
    "    gamma: float,\n",
    "    n_iters: int,\n",
    "    max_iters_policy_eval: int = 10000,\n",
    "    random_init: bool = True,\n",
    ") -> plt.Figure:\n",
    "    \"\"\"Policy Iteration algorithm\n",
    "\n",
    "    :param env: a FrozenLakeEnv environment\n",
    "    :param gamma: discount factor\n",
    "    :param n_iter: number of loop iterations\n",
    "    :param max_iters_policy_eval: maximum iterations of policy evaluation for each loop iteration\n",
    "    :param random_init: initialize value function randomly or as all zeros?\n",
    "    :return: Figure containing final value function and learning curves\n",
    "    \"\"\"\n",
    "\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    mdp = env.unwrapped.P\n",
    "    rng = np.random.default_rng(seed=SEED)\n",
    "    if random_init:\n",
    "        value = rng.normal(loc=0, scale=0.5, size=(n_states,))\n",
    "    else:\n",
    "        value = np.zeros(shape=n_states)\n",
    "    pi_prob = (\n",
    "        np.ones(shape=(n_states, n_actions)) / n_actions\n",
    "    )  # contains the action probabilities for each state\n",
    "    pi = np.zeros(shape=n_states, dtype=np.int64)  # contains the actual actions\n",
    "\n",
    "    # setup plotting\n",
    "    fig, (ax_value, ax_success, ax_change) = plt.subplots(1, 3, figsize=(12,5))\n",
    "    fig.subplots_adjust(wspace=0.3)\n",
    "    grid = (nrow, ncol) = (env.unwrapped.nrow, env.unwrapped.ncol)\n",
    "    image_value = ax_value.imshow(value.reshape(grid), vmin=0.0, vmax=0.9)\n",
    "    fig.colorbar(image_value)\n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            desc = env.unwrapped.desc[i, j].decode()\n",
    "            ax_value.text(j, i, desc, ha=\"center\", va=\"center\", color=\"w\")\n",
    "    ax_success.set(xlabel=\"Iterations\", ylabel=\"Success Rate\")\n",
    "    ax_success.set_xlim(0, n_iters)\n",
    "    ax_success.set_ylim(0.0, 1.0)\n",
    "    line_success, *_ = ax_success.plot([], [])\n",
    "    ax_change.set(xlabel=\"Iterations\", ylabel=\"Mean Delta\")\n",
    "    ax_change.set_xlim(0, n_iters)\n",
    "    ax_change.set_ylim(1e-7, 1.0)\n",
    "    ax_change.set_yscale(\"log\")\n",
    "    line_change, *_ = ax_change.plot([], [])\n",
    "\n",
    "    env.reset(seed=int(rng.integers(2**32)))\n",
    "\n",
    "    iters = []\n",
    "    success_rates = []\n",
    "    mean_delta_values = []\n",
    "    for iter in range(n_iters):\n",
    "        previous_value = np.copy(value)\n",
    "        iters.append(iter + 1)\n",
    "\n",
    "        # run policy evaluation\n",
    "        value = policy_evaluation(mdp, pi_prob, value, gamma, max_iters_policy_eval)\n",
    "\n",
    "        # run policy improvement\n",
    "        pi, pi_prob = policy_improvement(mdp, value, gamma)\n",
    "\n",
    "        # evaluate policy success rate\n",
    "        success_rate = measure_policy_success(env, pi, n_eval=100)\n",
    "        success_rates.append(success_rate)\n",
    "\n",
    "        mean_delta_value = np.abs(value - previous_value).mean()\n",
    "        mean_delta_values.append(mean_delta_value)\n",
    "\n",
    "        # update plots\n",
    "        display.clear_output(wait=True)\n",
    "        image_value.set_data(value.reshape(grid))\n",
    "        line_success.set_data(iters, success_rates)\n",
    "        line_change.set_data(iters, mean_delta_values)\n",
    "        display.display(fig)\n",
    "\n",
    "    # clear duplicate plot\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    return fig, success_rates, mean_delta_values, image_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "45ed264d1646ca65230dd0e4aa2f4f59",
     "grade": false,
     "grade_id": "cell-ae8a754fada2e5b0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# Run Policy Iteration\n",
    "fig, success_rates, mean_delta_values, image_value = policy_iteration(\n",
    "    env,\n",
    "    gamma=0.99,\n",
    "    n_iters=10,\n",
    "    max_iters_policy_eval=10000,\n",
    "    random_init=True,\n",
    ")\n",
    "save_figure(fig, \"policy_iteration\")\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "82e024b8b440ef4e75a6a7935fd0ae9e",
     "grade": true,
     "grade_id": "test_ex_1",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# ID: test_ex_1 - possible points: 4\n",
    "\n",
    "# Test cells for policy evaluation and policy improvement\n",
    "test_env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", render_mode=\"rgb_array\")\n",
    "test_env.reset(seed=SEED)\n",
    "test_mdp = test_env.unwrapped.P\n",
    "test_n_states = test_env.observation_space.n\n",
    "test_n_actions = test_env.action_space.n\n",
    "\n",
    "# Test policy_evaluation\n",
    "test_pi_prob = np.ones(shape=(test_n_states, test_n_actions)) / test_n_actions\n",
    "test_value = np.zeros(test_n_states)\n",
    "test_value_result = policy_evaluation(test_mdp, test_pi_prob, test_value, gamma=0.99, max_iters=1000)\n",
    "assert test_value_result.shape == (test_n_states,), \"policy_evaluation should return value function of correct shape\"\n",
    "assert np.all(test_value_result >= 0), \"All values should be non-negative for this environment\"\n",
    "\n",
    "# Test policy_improvement  \n",
    "test_pi, test_pi_prob = policy_improvement(test_mdp, test_value_result, gamma=0.99)\n",
    "assert test_pi.shape == (test_n_states,), \"policy_improvement should return policy of correct shape\"\n",
    "assert test_pi_prob.shape == (test_n_states, test_n_actions), \"policy_improvement should return action probabilities of correct shape\"\n",
    "assert np.allclose(np.sum(test_pi_prob, axis=1), 1.0), \"Action probabilities should sum to 1 for each state\"\n",
    "\n",
    "print(\"✓ Basic tests passed for policy_evaluation and policy_improvement\")\n",
    "\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "248ad5289f40c05abb32833ad9baac11",
     "grade": false,
     "grade_id": "cell-9c8c6713459e201a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Self-Test Questions (optional, not graded)\n",
    "\n",
    "Play with the hyperparameters for this algorithm if you wish.\n",
    "\n",
    "In this implementation, we stop policy evaluation when we reach convergence or after at most `max_iters_policy_eval` iterations.\n",
    "Set `max_iters_policy_eval` to $1$, meaning that we only update the values once before using it to improve the policy again.\n",
    "How much do you need to increase `n_iters` to reach the same level of success as before?\n",
    "\n",
    "What happens as you increase or decrease `gamma`, the discount factor?\n",
    "How much can you _decrease_ the `gamma` before the success rate starts to suffer?\n",
    "\n",
    "Set `gamma` to $1.0$.\n",
    "Are the final values for the holes (marked \"H\") $0$, or something else?\n",
    "If they aren't $0$, perhaps you forgot to take into account terminal states (`done`) during policy evaluation.\n",
    "\n",
    "Be sure to set to the values back to their original values before generating the figures for submission!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8cf1349a0c343f7b06e1543bef954117",
     "grade": false,
     "grade_id": "cell-5562b1fe30a7f979",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 2: Value Iteration\n",
    "Next, we will have a look at Value Iteration.\n",
    "It is very similar to Policy Iteration, except the sum over the policy $\\pi(a \\mid s)$ has been replaced by a max over actions.\n",
    "In other words, we no longer consider the current policy during the evaluation step, and instead simply assume that the current policy is always optimal with respect to the current value function.\n",
    "The **pseudocode** looks as follows\n",
    "\n",
    "---\n",
    "- **Initialize** $V_{(0)}^\\ast(s)$ randomly for all $s$\n",
    "\n",
    "- **Repeat** for $i=1, 2, \\dots$\n",
    "\n",
    "    \\begin{equation*}\n",
    "        V^\\ast_{(i)}(s) \\leftarrow \\underset{a}{\\max} \\sum_{s'} P(s' \\mid s,a) \\Big( r(s,a,s') + \\overline{d} \\gamma \\, V^\\ast_{(i-1)}(s') \\Big)\n",
    "    \\end{equation*}\n",
    "\n",
    "- **Until convergence**\n",
    "---\n",
    "\n",
    "Again, note the addition of $\\overline{d}$ to the formula from the lecture, to account for terminal states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6f57ce38f845228e7a8cfe7b3f7ec489",
     "grade": false,
     "grade_id": "cell-c5fa4141b16f0ec3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 2.1 (4 Points)\n",
    "In this task you have to implement the core algorithm of **value iteration** (4 points), marked with `# YOUR CODE HERE`.\n",
    "Since the policy is implicitly defined as the optimal policy for the current value function, you must also compute the updated policy as part of value iteration.\n",
    "\n",
    "After you have finished implementing the corresponding functions, execute the code cell.\n",
    "Your algorithm should be able to solve the task with around ~65% success rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "task_2_1_value_iteration",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3055dd14d2309ecfeab76ec7e90afd76",
     "grade": false,
     "grade_id": "exercise_2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "points": 4
   },
   "outputs": [],
   "source": [
    "def value_iteration(\n",
    "    env: FrozenLakeEnv,\n",
    "    gamma: float,\n",
    "    n_iters: int,\n",
    "    random_init: bool = True,\n",
    ") -> plt.Figure:\n",
    "    \"\"\"Value Iteration algorithm\n",
    "\n",
    "    :param env: a FrozenLakeEnv environment\n",
    "    :param gamma: discount factor\n",
    "    :param n_iter: number of loop iterations\n",
    "    :param random_init: initialize value function randomly or as all zeros?\n",
    "    :return: Figure containing final value function and learning curves\n",
    "    \"\"\"\n",
    "\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    mdp = env.unwrapped.P\n",
    "    rng = np.random.default_rng(seed=SEED)\n",
    "    if random_init:\n",
    "        value = rng.normal(loc=0, scale=0.5, size=(n_states,))\n",
    "    else:\n",
    "        value = np.zeros(shape=n_states)\n",
    "    pi = np.zeros(shape=n_states, dtype=np.int64)  # contains the actual actions\n",
    "\n",
    "    # setup plotting\n",
    "    fig, (ax_value, ax_success, ax_change) = plt.subplots(1, 3, figsize=(12,5))\n",
    "    fig.subplots_adjust(wspace=0.3)\n",
    "    grid = (nrow, ncol) = (env.unwrapped.nrow, env.unwrapped.ncol)\n",
    "    image_value = ax_value.imshow(value.reshape(grid), vmin=0.0, vmax=0.9)\n",
    "    fig.colorbar(image_value)\n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            desc = env.unwrapped.desc[i, j].decode()\n",
    "            ax_value.text(j, i, desc, ha=\"center\", va=\"center\", color=\"w\")\n",
    "    ax_success.set(xlabel=\"Iterations\", ylabel=\"Success Rate\")\n",
    "    ax_success.set_xlim(0, n_iters)\n",
    "    ax_success.set_ylim(0.0, 1.0)\n",
    "    line_success, *_ = ax_success.plot([], [])\n",
    "    ax_change.set(xlabel=\"Iterations\", ylabel=\"Mean Delta\")\n",
    "    ax_change.set_xlim(0, n_iters)\n",
    "    ax_change.set_ylim(1e-7, 1.0)\n",
    "    ax_change.set_yscale(\"log\")\n",
    "    line_change, *_ = ax_change.plot([], [])\n",
    "\n",
    "    env.reset(seed=int(rng.integers(2**32)))\n",
    "\n",
    "    iters = []\n",
    "    success_rates = []\n",
    "    mean_delta_values = []\n",
    "    for iter in range(0, n_iters):\n",
    "        previous_value = np.copy(value)\n",
    "        iters.append(iter)\n",
    "\n",
    "        # hint: Here, you will need to fill in the new policy pi and value function v for all states s.\n",
    "        # I.e., you need to update pi[s], v[s] for all s.\n",
    "        # get a list of possible transitions using mdp[state][action], which returns a list of\n",
    "        # tuples of (transition_prob, next_state, reward, done)\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # Evaluate policy success rate\n",
    "        success_rate = measure_policy_success(env, pi, n_eval=100)\n",
    "        success_rates.append(success_rate)\n",
    "\n",
    "        mean_delta_value = np.abs(value - previous_value).mean()\n",
    "        mean_delta_values.append(mean_delta_value)\n",
    "\n",
    "        # update plots\n",
    "        display.clear_output(wait=True)\n",
    "        image_value.set_data(value.reshape(grid))\n",
    "        line_success.set_data(iters, success_rates)\n",
    "        line_change.set_data(iters, mean_delta_values)\n",
    "        display.display(fig)\n",
    "\n",
    "    # clear duplicate plot\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    return fig, success_rates, mean_delta_values, image_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "337ba4f4f9998f7fc0dda62e1facb6fe",
     "grade": false,
     "grade_id": "cell-286ba919ea209f0c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# Run Value Iteration\n",
    "fig, success_rates, mean_delta_values, image_value = value_iteration(env, gamma=0.99, n_iters=20, random_init=True)\n",
    "save_figure(fig, \"value_iteration\")\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c39f0f754dd55e5868098baedec15a15",
     "grade": true,
     "grade_id": "test_ex_2",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# ID: test_ex_2 - possible points: 4\n",
    "\n",
    "# Test cell for value_iteration\n",
    "test_env_vi = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", render_mode=\"rgb_array\")\n",
    "test_env_vi.reset(seed=SEED)\n",
    "\n",
    "# Run value iteration with a small number of iterations\n",
    "test_fig, test_success_rates, test_mean_delta_values, test_image_value = value_iteration(test_env_vi, gamma=0.99, n_iters=5, random_init=False)\n",
    "assert isinstance(test_fig, plt.Figure), \"value_iteration should return a matplotlib Figure\"\n",
    "\n",
    "print(\"✓ Basic tests passed for value_iteration\")\n",
    "\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e6c85c62f557d4d511691cb8f80e1390",
     "grade": false,
     "grade_id": "cell-06fac1f6d8b428fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Self-Test Questions (optional, not graded)\n",
    "\n",
    "Here we initialize a value function randomly and then run $20$ iterations of value iteration.\n",
    "How is this different from running $20$ iterations of policy iteration (the previous task) with `max_iters_policy_eval` set to $1$?\n",
    "Which one converges faster?\n",
    "\n",
    "The heat map of the value function for policy iteration is noticably lighter (i.e. higher values) than the value function for value iteration.\n",
    "Is there a reason why policy iteration converges to higher state values than value iteration, or can this discrepancy be resolved?\n",
    "\n",
    "Play with the hyperparameters for this algorithm if you wish.\n",
    "\n",
    "What happens if the value function is initialized uniformly to $0$ instead of randomly (set `random_init=False`)?\n",
    "Why does convergence slow down?\n",
    "\n",
    "Be sure to set to the values back to their original values before generating the figures for submission!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "84f6c07392d245533e61730e5e30c262",
     "grade": false,
     "grade_id": "cell-38bc5b20287b8847",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 3: Q-Learning\n",
    "In Policy Iteration and Value Iteration, we assume that we have knowledge about the problem's underlying dynamics (the dictionary of transitions) and the reward function.\n",
    "This information is usually not available in practice.\n",
    "Instead, we must solve the problem using only the trajectories we have already seen, and ensure we collect sufficient new trajectories through exploration.\n",
    "One common and effective algorithm for solving such problems is Q-Learning.\n",
    "\n",
    "The **pseudocode** look as follows:\n",
    "\n",
    "---\n",
    "- **Initialize** $Q_{(0)}(s, a)$ randomly for all $s$ and $a$\n",
    "\n",
    "- **Repeat** for $i=1, 2, \\dots$\n",
    "    - sample an action $a$ using the exploration strategy and get the next state $s'$ and associated reward $r$\n",
    "\n",
    "    \\begin{align*}\n",
    "        \\delta &= r(s, a) + \\overline{d} \\gamma \\underset{a'}{\\max} Q_{(i-1)}(s', a') - Q_{(i-1)}(s, a) \\\\\n",
    "        Q_{(i)}(s, a) &= Q_{(i-1)}(s, a) + \\alpha \\delta\n",
    "    \\end{align*}\n",
    "    \n",
    "    - If $s'$ is terminal: reset environment and sample new initial state $s'$\n",
    "    - Set $ s \\leftarrow s' $\n",
    "\n",
    "- **Until convergence**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a20ddf437216abc30cde88ee680484d",
     "grade": false,
     "grade_id": "cell-1d9dc3024552c597",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 3.1 (4 Points)\n",
    "Finally, you have to implement the core algorithm of **QLearning** (4 points), marked with `# YOUR CODE HERE`.\n",
    "Just like in the pseudocode above, you should first compute the TD-Error $\\delta$ and then update the Q function.\n",
    "\n",
    "After you have finished implementing the corresponding functions, execute the code cell.\n",
    "If implemented correctly, your algorithm should be able to solve the task with something between 60% and 80% success rate, although the variability is high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "task_3_1_q_learning",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "823937499048858b2fcdb04a1ad5fa33",
     "grade": false,
     "grade_id": "exercise_3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "points": 4
   },
   "outputs": [],
   "source": [
    "def q_learning(\n",
    "    env: FrozenLakeEnv,\n",
    "    alpha_init: float,\n",
    "    gamma: float,\n",
    "    n_steps: int,\n",
    "    t_decay: int | float,\n",
    "    random_init: bool = True,\n",
    ") -> plt.Figure:\n",
    "    \"\"\"Q-Learning algorithm\n",
    "\n",
    "    :param env: a FrozenLakeEnv environment\n",
    "    :param alpha: initial learning rate (decays over time)\n",
    "    :param gamma: discount factor\n",
    "    :param n_steps: total number of steps the agent takes in the environment\n",
    "    :param t_decay: hyperparameter that controls the rate of exploration decay\n",
    "    :param random_init: initialize Q function randomly or as all zeros?\n",
    "    :return: Figure containing final value function and learning curve\n",
    "    \"\"\"\n",
    "    eval_every = n_steps // 200\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    if random_init:\n",
    "        q = rng.normal(loc=0, scale=0.5, size=(n_states, n_actions))\n",
    "    else:\n",
    "        q = np.zeros((n_states, n_actions))\n",
    "    value = np.max(q, axis=1)\n",
    "\n",
    "    # setup plotting\n",
    "    fig, (ax_value, ax_success) = plt.subplots(1, 2, figsize=(12,5))\n",
    "    fig.subplots_adjust(wspace=0.3)\n",
    "    grid = (nrow, ncol) = (env.unwrapped.nrow, env.unwrapped.ncol)\n",
    "    image_value = ax_value.imshow(value.reshape(grid), vmin=0.0, vmax=0.9)\n",
    "    fig.colorbar(image_value)\n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            desc = env.unwrapped.desc[i, j].decode()\n",
    "            ax_value.text(j, i, desc, ha=\"center\", va=\"center\", color=\"w\")\n",
    "    ax_success.set(xlabel=\"# Steps\", ylabel=\"Success Rate\")\n",
    "    ax_success.set_xlim(0, n_steps)\n",
    "    ax_success.set_ylim(0.0, 1.0)\n",
    "    line_success, *_ = ax_success.plot([], [])\n",
    "\n",
    "    state, info = env.reset(seed=int(rng.integers(2**32)))\n",
    "\n",
    "    ts = []\n",
    "    success_rates = []\n",
    "    for t in range(n_steps):\n",
    "        epsilon = max(0.01, 1 - t / t_decay)\n",
    "        alpha = max(0.01, alpha_init * (1 - t / n_steps))\n",
    "\n",
    "        # select action using epsilon-greedy method\n",
    "        # hint: if random() < epsilon, return a random action\n",
    "        # hint: otherwise return the greedy action, i.e. the one with the highest q\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # compute delta and update Q function\n",
    "        # hint: Here, you will need to update the Q function with the data from the\n",
    "        # latest transition above\n",
    "        # hint: instead of done, use the `terminated` signal here to check if the next state is terminal\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        if terminated or truncated:\n",
    "            state, info = env.reset()\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "        # evaluate policy success rate\n",
    "        if t % eval_every == 0:\n",
    "            value = np.max(q, axis=1)\n",
    "            pi = np.argmax(q, axis=1)\n",
    "\n",
    "            ts.append(t)\n",
    "            success_rate = measure_policy_success(env, pi, n_eval=100)\n",
    "            success_rates.append(success_rate)\n",
    "\n",
    "            # update plots\n",
    "            display.clear_output(wait=True)\n",
    "            image_value.set_data(value.reshape(grid))\n",
    "            line_success.set_data(ts, success_rates)\n",
    "            display.display(fig)\n",
    "\n",
    "    # clear duplicate plot\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    return fig, success_rates, image_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7275f709a9cc96d410df6fc468fddc5e",
     "grade": false,
     "grade_id": "cell-73174a5d88dd39c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# Run QLearning\n",
    "small_env: FrozenLakeEnv = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", render_mode=\"rgb_array\")\n",
    "fig, success_rates, image_value = q_learning(\n",
    "    small_env,\n",
    "    alpha_init=0.2,  # initial learning rate (decays over time)\n",
    "    gamma=0.99,  # discount factor\n",
    "    n_steps=40000,  # total number of steps the agent takes in the environment\n",
    "    t_decay=20000,  # controls the rate of exploration decay\n",
    "    random_init=False,  # initialize Q function randomly or with 0?\n",
    ")\n",
    "save_figure(fig, \"q_learning\")\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "20e7c62b54a70eb3817b761571f67b03",
     "grade": true,
     "grade_id": "test_ex_3",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# ID: test_ex_3 - possible points: 4\n",
    "\n",
    "# Test cell for q_learning\n",
    "test_env_ql = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", render_mode=\"rgb_array\")\n",
    "test_env_ql.reset(seed=SEED)\n",
    "\n",
    "# Run q_learning with a small number of steps\n",
    "test_fig_ql, test_success_rates, test_image_value = q_learning(test_env_ql, alpha_init=0.2, gamma=0.99, n_steps=1000, t_decay=500, random_init=False)\n",
    "assert isinstance(test_fig_ql, plt.Figure), \"q_learning should return a matplotlib Figure\"\n",
    "\n",
    "print(\"✓ Basic tests passed for q_learning\")\n",
    "\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b34a9659fff8c3cdd23ba8e1068cfbdf",
     "grade": false,
     "grade_id": "cell-3841ad99fce7d5ed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Self-Test Questions (optional, not graded)\n",
    "\n",
    "Why do you think that the success rate of Q-learning varies more during learning than it does with policy iteration or value iteration?\n",
    "\n",
    "When we measure success, we set the policy to be just the argmax over all actions in each state.\n",
    "Why does this approach hurt exploration, especially during the initial stages of training?\n",
    "\n",
    "Play with the hyperparameters for this algorithm if you wish.\n",
    "Can you find better hyperparameters?\n",
    "Change the random seed as well, to see if your hyperparameters are still good for different random conditions.\n",
    "\n",
    "What happens if the Q function is initialized randomly instead of uniformly to $0$?\n",
    "Why does training become much slower?\n",
    "\n",
    "Be sure to set to the values back to their original values before generating the figures for submission!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "485b2536cf4aa40194fb65c188b58d4a",
     "grade": false,
     "grade_id": "cell-4df32ad36f417ce2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Additional Task 3.2 (not graded)\n",
    "\n",
    "For the previous task, we used the 4x4 version of the FrozenLake environment because Q-Learning requires many samples and converges quite slowly.\n",
    "Run the code cell below to try your Q-Learning implementation on the full 8x8 version of the environment.\n",
    "\n",
    "The algorithm should reach a success rate around $60%$, but sometimes the success rate drops sharply during training.\n",
    "In my experimentation, the algorithm doesn't learn with `SEED = 6` (leave all other hyperparameters at their defaults).\n",
    "In addition, there are regions of the environment (e.g. the bottom left) where the Q function still differs from the optimal Q function computed in questions 1 and 2, with no sign that they are improving.\n",
    "Given this, it is reasonable to wonder if this algorithm will be able to find an optimal policy.\n",
    "\n",
    "**Question:** Under what conditions (if any) is Q-Learning able to find an optimal policy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8724c7c530cb242afa290a629c635b2d",
     "grade": false,
     "grade_id": "cell-92793099cec0f8f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# Run QLearning on the 8x8 environment\n",
    "SEED = 1\n",
    "_ = q_learning(env, gamma=0.99, alpha_init=0.2, n_steps=400000, t_decay=200000, random_init=False)\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
