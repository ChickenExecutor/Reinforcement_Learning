{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44531cd0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "51d0f0cbd00c73c5f51e77690cb690d7",
     "grade": false,
     "grade_id": "cell-4e7b1b0b06eb1947",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise 4: Imitation Learning and Offline Reinforcement Learning\n",
    "\n",
    "---\n",
    "\n",
    "> **Reinforcement Learning** in Winter Semester 2025/2026\n",
    ">\n",
    "> - Prof. Gerhard Neumann and Prof. Rudolf Lioutikov\n",
    "> - Instructor for this exercise: Hongyi Zhou (hongyi.zhou@kit.edu)\n",
    "> \n",
    "> ⚠️ For general questions about the exercises, please post in the Ilias forum so we can answer them once for all students. Only use email for individual questions whose answers are not relevant to all students.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5277831f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f4e86756b92165c4f01842307fb039f0",
     "grade": false,
     "grade_id": "cell-b6cd880e81dc1595",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Group Submission\n",
    "\n",
    "The exercise sheets can be submitted in groups of up to **3 students**. **Only one member in the group is required to upload the final version via Ilias**.\n",
    "\n",
    "Groups are automatically recorded, so **please enter the u-identifiers of your group members in the following cell.** If your group consists of only 2 students, or if you are submitting alone, leave the remaining fields empty. Here is an **example** for a group consisting of uabcd and uefgh:\n",
    "\n",
    "_U-identifiers of group members:_\n",
    "\n",
    "_Member 1: uabcd_\n",
    "\n",
    "_Member 2: uefgh_\n",
    "\n",
    "_Member 3:_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790095cd",
   "metadata": {},
   "source": [
    "U-identifiers of group members:\n",
    "\n",
    "Member 1:\n",
    "\n",
    "Member 2:\n",
    "\n",
    "Member 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34484f75",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "832ef2866ea9b6501c1f642601d2a122",
     "grade": false,
     "grade_id": "cell-109e42bfcc7f2efc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Auto-grading\n",
    "\n",
    "We use an auto-grading system that automatically analyzes your submitted Jupyter Notebooks and checks them for correctness using hidden tests. These tests determine the points you receive for the exercise sheet.\n",
    "\n",
    "To ensure auto-grading works smoothly, please note the following:\n",
    "\n",
    "- The notebook must have the filename \"ex_04_offline_rl.ipynb\"\n",
    "- Upload Jupyter Notebook on Ilias (not as a zip!)\n",
    "- Before submitting a notebook, test that everything runs without errors from start to finish.\n",
    "- Cells marked with \"##### DO NOT CHANGE #####\" must not be edited or deleted\n",
    "- Your solution must be entered in the correct cell (marked with \"# YOUR CODE HERE\").\n",
    "  - Please delete the **NotImplementedError!**\n",
    "- Generally, **do not delete any cells** and **do not add any cells**. The cells where your solution should be entered already exist (marked with \"# YOUR CODE HERE\").\n",
    "- There may be seemingly empty cells that are also marked with \"##### DO NOT CHANGE #####\". These must also not be edited or deleted.\n",
    "  - If you do modify them, auto-grading will not work and you will receive no points.\n",
    "  - We will be strict about this and make no exceptions if someone modifies cells clearly marked as readonly!\n",
    "- The Jupyter Notebooks have inline tests (visible to you) that check your result for basic correctness.\n",
    "  - These are primarily for you to identify and correct errors.\n",
    "  - However, the inline tests you can see in the notebook are not the tests used for grading!\n",
    "  - The inline tests are a necessary but not sufficient condition to receive points when grading the task!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c8021c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b96784f71896b697141925dfb4195806",
     "grade": false,
     "grade_id": "cell-a5f13572f2ff72a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Local Setup\n",
    "\n",
    "If you prefer to work locally, see the following instructions for setting up Python in a virtual environment.\n",
    "You can then ignore the instructions in \"Colab Setup\".\n",
    "\n",
    "If you haven't yet, create a [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html) environment using:\n",
    "```\n",
    "conda create --name rl_exercises\n",
    "conda activate rl_exercises\n",
    "```\n",
    "Torch recommends installation using conda rather than pip, so run:\n",
    "```\n",
    "conda install pytorch cpuonly -c pytorch\n",
    "```\n",
    "If you have a CUDA-enabled GPU and would like to use it, visit [the installation page](https://pytorch.org/get-started/locally/) to see the options available for different CUDA versions.\n",
    "The remaining dependencies can be installed with pip:\n",
    "```\n",
    "pip install minari gymnasium[mujoco] matplotlib numpy tqdm\n",
    "```\n",
    "\n",
    "Even if you are running the Jupyter notebook locally, please run the code cells in **Colab Setup**, because they define some global variables required later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6f2281",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b731b5f25ff59b48d65ca86b7bf494fd",
     "grade": false,
     "grade_id": "cell-a1c56d3b3a0b2f96",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Colab Setup\n",
    "\n",
    "Google Colab provides you with a temporary environment for python programming.\n",
    "While this conveniently works on any platform and internally handles dependency issues and such, it also requires you to set up the environment from scratch every time.\n",
    "The \"Colab Setup\" section below will be part of **every** exercise and contains utility that is needed before getting started.\n",
    "\n",
    "There is a timeout of about ~12 hours with Colab while it is active (and less if you close your browser window).\n",
    "Any changes you make to the Jupyter notebook itself should be saved to your Google Drive.\n",
    "We also save all recordings and logs in it by default so that you won't lose your work in the event of an instance timeout.\n",
    "However, you will need to re-mount your Google Drive and re-install packages with every new instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfae86a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d046a4962d1dc5eccda50e91801f1977",
     "grade": false,
     "grade_id": "cell-0740323206b19df9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# Colab setup - install dependencies\n",
    "try:\n",
    "    import google.colab\n",
    "    COLAB = True\n",
    "    %pip install -q minari gymnasium[mujoco] matplotlib numpy tqdm torch\n",
    "except ImportError:\n",
    "    COLAB = False\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8a073b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b7f0e30c6efe7f4cfdcf55c5594b00a",
     "grade": false,
     "grade_id": "cell-a308ccba99650463",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Part 1: Self-Attention\n",
    "\n",
    "In this exercise, you will implement a function to compute the most fundamental variant of self-attention, known as the scaled dot-product attention. This mechanism is a cornerstone in many state-of-the-art models, particularly in natural language processing and imitation learning.\n",
    "\n",
    "Your task is to fill in a function that computes the scaled dot-product attention of input vectors. The input to your function will be three matrices: Queries (Q), Keys (K), and Values (V), each with a shape of $(n, d_k)$, where $n$ represents the number of embeddings and $d_k$ represents the embedding dimension.\n",
    "\n",
    "The scaled dot-product attention is mathematically represented as:\n",
    "$$\n",
    "\\text{Attention}(Q,K,V) = \\text{softmax}(\\frac{Q K^T}{\\sqrt{d_k}})V\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3a2272",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75b744667129897bb6ead122cb835e80",
     "grade": false,
     "grade_id": "cell-dc5d0a03b86b5d43",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float32\n",
    "\n",
    "def self_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Implement the self-attention mechanism.\n",
    "\n",
    "    Parameters:\n",
    "    Q (torch.Tensor): Query matrix\n",
    "    K (torch.Tensor): Key matrix\n",
    "    V (torch.Tensor): Value matrix\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Output of self-attention mechanism\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "for _ in tqdm(range(2000)):\n",
    "    inputs = torch.randn(10, 20).to(DEVICE).to(dtype)\n",
    "    d_k = inputs.shape[-1]\n",
    "\n",
    "    ### Initialize the weights matrix for Q, K, V\n",
    "    W_q = torch.randn(d_k, d_k).to(DEVICE).to(dtype)\n",
    "    W_k = torch.randn(d_k, d_k).to(DEVICE).to(dtype)\n",
    "    W_v = torch.randn(d_k, d_k).to(DEVICE).to(dtype)\n",
    "\n",
    "    ### Calculate the Q, K, V from embeddings\n",
    "    Q = torch.matmul(inputs, W_q)\n",
    "    K = torch.matmul(inputs, W_k)\n",
    "    V = torch.matmul(inputs, W_v)\n",
    "\n",
    "    # Call your self_attention function\n",
    "    output_vectors = self_attention(Q, K, V)\n",
    "\n",
    "    # Verify the results\n",
    "    self_attention_builtin = F.scaled_dot_product_attention(Q, K, V)\n",
    "    assert torch.allclose(self_attention_builtin, output_vectors, atol=1e-4),\"\\n The implementation seems to be incorrect!\"\n",
    "\n",
    "print(\"\\n The implementation seems to be correct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54b9cb4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9dc82ff2c6fa0effcd413bdc7f122b0",
     "grade": true,
     "grade_id": "cell-455f4d796657b514",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# ID: cell-455f4d796657b514 - possible points: 4\n",
    "\n",
    "# Test self-attention implementation\n",
    "print(\"Testing self-attention...\")\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "test_Q = torch.randn(10, 20).to(DEVICE).to(dtype)\n",
    "test_K = torch.randn(10, 20).to(DEVICE).to(dtype)\n",
    "test_V = torch.randn(10, 20).to(DEVICE).to(dtype)\n",
    "\n",
    "result = self_attention(test_Q, test_K, test_V)\n",
    "expected = F.scaled_dot_product_attention(test_Q, test_K, test_V)\n",
    "\n",
    "assert result.shape == test_V.shape, f\"Output shape {result.shape} should match V shape {test_V.shape}\"\n",
    "assert torch.allclose(result, expected, atol=1e-4), \"Self-attention output does not match expected\"\n",
    "\n",
    "print(\"✓ Self-attention passed basic tests!\")\n",
    "\n",
    "\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c62a44a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "953ab980ef51dfea6af3f0dd32f82193",
     "grade": false,
     "grade_id": "cell-2fe2422b3d56b569",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Imports and Utilities for Part 2 and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067c4f1d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d0a7c665f82f34c50b1b3d09c3a0ba4",
     "grade": false,
     "grade_id": "cell-32406e7d46d8b066",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "import os\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "\n",
    "import minari\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "def update_plot(iterations, rewards, algorithm='AWAC'):\n",
    "    \"\"\"\n",
    "    Update the training progress plot with new iterations and rewards.\n",
    "    \"\"\"\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(iterations, rewards, label=f'{algorithm}')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Episode Return')\n",
    "    plt.title(f'Learning Curve of the {algorithm} Algorithm')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Sets a fixed random seed for reproducibility.\n",
    "    \"\"\"\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "def set_seed(\n",
    "    seed: int, env: Optional[gym.Env] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Sets a fixed random seed for various modules and the environment (if provided)\n",
    "    to ensure reproducibility.\n",
    "\n",
    "    Parameters:\n",
    "    - seed (int): The random seed to set.\n",
    "    - env (Optional[gym.Env]): The gym environment to set the seed for.\n",
    "    Defaults to None.\n",
    "\n",
    "    Description:\n",
    "    - If an environment is provided, its seed and action space seed are set to\n",
    "    ensure consistent results in the environment.\n",
    "    - Sets the PYTHONHASHSEED environment variable, which controls the randomness\n",
    "    of Python's hash-based operations.\n",
    "    - Initializes the random seeds for NumPy, Python's built-in random module,\n",
    "    and PyTorch to ensure consistent random number generation across these libraries.\n",
    "    \"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    if env is not None:\n",
    "        # Gymnasium: seed at reset-time, plus spaces if you want deterministic sampling.\n",
    "        try:\n",
    "            env.action_space.seed(seed)\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            env.observation_space.seed(seed)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "def compute_mean_std_from_minari(minari_dataset) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute the mean and standard deviation of observations and actions from a Minari dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - minari_dataset: The Minari dataset to compute statistics from.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        (state_mean, state_std, action_mean, action_std)\n",
    "    \"\"\"\n",
    "    all_observations = []\n",
    "    all_actions = []\n",
    "    for ep in minari_dataset.iterate_episodes():\n",
    "        obs = np.asarray(ep.observations)\n",
    "        act = np.asarray(ep.actions)\n",
    "        all_observations.append(obs)\n",
    "        all_actions.append(act)\n",
    "\n",
    "    all_observations = np.concatenate(all_observations, axis=0)\n",
    "    all_actions = np.concatenate(all_actions, axis=0)\n",
    "\n",
    "    state_mean = all_observations.mean(axis=0)\n",
    "    state_std = all_observations.std(axis=0) + 1e-8  # Add small epsilon for numerical stability\n",
    "    action_mean = all_actions.mean(axis=0)\n",
    "    action_std = all_actions.std(axis=0) + 1e-8\n",
    "\n",
    "    return state_mean, state_std, action_mean, action_std\n",
    "\n",
    "def wrap_env(\n",
    "    env: gym.Env,\n",
    "    state_mean: Union[np.ndarray, float] = 0.0,\n",
    "    state_std: Union[np.ndarray, float] = 1.0,\n",
    ") -> gym.Env:\n",
    "    \"\"\"\n",
    "    Wraps a gym environment to normalize its states.\n",
    "\n",
    "    Parameters:\n",
    "    - env (gym.Env): The gym environment to wrap.\n",
    "    - state_mean (Union[np.ndarray, float]): The mean used for state normalization, can be a scalar or an array. Defaults to 0.0.\n",
    "    - state_std (Union[np.ndarray, float]): The standard deviation used for state normalization, can be a scalar or an array. Defaults to 1.0.\n",
    "\n",
    "    Returns:\n",
    "    - gym.Env: The wrapped gym environment with state normalization.\n",
    "\n",
    "    Description:\n",
    "    - Normalizes the states of the environment by subtracting the mean and dividing by the standard deviation.\n",
    "    - Utilizes the TransformObservation wrapper from gym.wrappers to apply the normalization.\n",
    "    \"\"\"\n",
    "    def normalize_state(state):\n",
    "        return (state - state_mean) / state_std\n",
    "\n",
    "    # Gymnasium >= 1.0 requires observation_space argument\n",
    "    env = gym.wrappers.TransformObservation(\n",
    "        env, normalize_state, observation_space=env.observation_space\n",
    "    )\n",
    "    return env\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_policy(\n",
    "        env: gym.Env, policy: torch.nn.Module, num_episodes: int = 10,\n",
    "        seed: int = 0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Evaluates a policy in the given environment over a specified number of episodes.\n",
    "\n",
    "    Parameters:\n",
    "    - env (gym.Env): The gym environment to evaluate the policy in.\n",
    "    - policy (torch.nn.Module): The policy model to evaluate.\n",
    "    - num_episodes (int): The number of episodes to run the evaluation for. Defaults to 10.\n",
    "    - seed (int): The random seed for environment reproducibility. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: An array of cumulative rewards received in each episode.\n",
    "\n",
    "    Description:\n",
    "    - Sets the seed for the environment for consistent episode generation.\n",
    "    - Puts the policy in evaluation mode to disable any training-specific operations like dropout.\n",
    "    - Runs the policy for a specified number of episodes, collecting total rewards per episode.\n",
    "    - Switches the policy back to training mode after evaluation.\n",
    "    - Uses PyTorch's no_grad context manager to disable gradient calculations, improving performance.\n",
    "    \"\"\"\n",
    "    policy.eval()\n",
    "    episode_rewards = []\n",
    "    for ep in range(num_episodes):\n",
    "        obs, _info = env.reset(seed=seed + ep)\n",
    "        episode_reward = 0.0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        while not (terminated or truncated):\n",
    "            action = policy.act(obs, device=DEVICE)\n",
    "            obs, reward, terminated, truncated, _info = env.step(action)\n",
    "            episode_reward += float(reward)\n",
    "        episode_rewards.append(episode_reward)\n",
    "\n",
    "    policy.train()\n",
    "    return np.array(episode_rewards)\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bb424a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "722ddfd6ce9b615ad4d53dc0c015a959",
     "grade": false,
     "grade_id": "cell-0cb8c726616d49c2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Policy Network (Actor)\n",
    "\n",
    "The following neural network is trained to represent the policy `p(a|s)`, the forward function return action **sampled** from predict distribution, along with its log-probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674299b0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef68a70f551513ebaea3e10e6f54f25b",
     "grade": false,
     "grade_id": "cell-67aee942aafb8432",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "class Actor(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        hidden_dim: int,\n",
    "        min_log_std: float = -20.0,\n",
    "        max_log_std: float = 2.0,\n",
    "        min_action: float = -1.0,\n",
    "        max_action: float = 1.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        An actor class that defines a policy network for reinforcement learning.\n",
    "\n",
    "        Parameters:\n",
    "        - state_dim (int): Dimension of the state space.\n",
    "        - action_dim (int): Dimension of the action space.\n",
    "        - hidden_dim (int): Dimension of the hidden layers in the neural network.\n",
    "        - min_log_std (float): Minimum logarithm of the standard deviation for the action distribution. Defaults to -20.0.\n",
    "        - max_log_std (float): Maximum logarithm of the standard deviation for the action distribution. Defaults to 2.0.\n",
    "        - min_action (float): Minimum action value. Defaults to -1.0.\n",
    "        - max_action (float): Maximum action value. Defaults to 1.0.\n",
    "\n",
    "        Description:\n",
    "        - Initializes a policy network using a multilayer perceptron (MLP) with ReLU activations.\n",
    "        - The network predicts the mean of the action distribution.\n",
    "        - The standard deviation of the action distribution is parameterized as a learnable parameter, clamped between specified min and max log values.\n",
    "        - The action range is bounded between specified min and max values.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._mlp = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "        )\n",
    "        self.register_buffer(\"_log_std\", torch.full((action_dim,), -1.0, dtype=torch.float32))\n",
    "        self._min_log_std = min_log_std\n",
    "        self._max_log_std = max_log_std\n",
    "        self._min_action = min_action\n",
    "        self._max_action = max_action\n",
    "\n",
    "    def _get_policy(self, state: torch.Tensor) -> torch.distributions.Distribution:\n",
    "        \"\"\"\n",
    "        Computes the policy distribution for a given state.\n",
    "\n",
    "        Parameters:\n",
    "        - state (torch.Tensor): The state tensor.\n",
    "\n",
    "        Returns:\n",
    "        - torch.distributions.Distribution: The policy distribution as a Normal distribution.\n",
    "\n",
    "        Description:\n",
    "        - Passes the state through the MLP to get the mean of the action distribution.\n",
    "        - Clamps the logarithm of the standard deviation within the specified range.\n",
    "        - Constructs a Normal distribution with the computed mean and standard deviation.\n",
    "        \"\"\"\n",
    "        mean = self._mlp(state)\n",
    "        log_std = self._log_std.clamp(self._min_log_std, self._max_log_std)\n",
    "        policy = torch.distributions.Normal(mean, log_std.exp())\n",
    "        return policy\n",
    "\n",
    "    def log_prob(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the log probability of a given action under the policy for a given state.\n",
    "\n",
    "        Parameters:\n",
    "        - state (torch.Tensor): The state tensor.\n",
    "        - action (torch.Tensor): The action tensor.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The log probability of the action under the policy.\n",
    "\n",
    "        Description:\n",
    "        - Retrieves the policy distribution for the given state.\n",
    "        - Calculates the log probability of the given action under this distribution.\n",
    "        - Sums the log probabilities across the action dimensions.\n",
    "        \"\"\"\n",
    "        policy = self._get_policy(state)\n",
    "        log_prob = policy.log_prob(action).sum(-1, keepdim=True)\n",
    "        return log_prob\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Defines the forward pass for the policy network.\n",
    "\n",
    "        Parameters:\n",
    "        - state (torch.Tensor): The state tensor.\n",
    "\n",
    "        Returns:\n",
    "        - Tuple[torch.Tensor, torch.Tensor]: A tuple of the action tensor and its log probability.\n",
    "\n",
    "        Description:\n",
    "        - Gets the policy distribution for the given state.\n",
    "        - Samples an action from this distribution using the reparameterization trick.\n",
    "        - Clamps the action within the specified range.\n",
    "        - Computes the log probability of the sampled action.\n",
    "        \"\"\"\n",
    "        policy = self._get_policy(state)\n",
    "        action = policy.rsample()\n",
    "        action.clamp_(self._min_action, self._max_action)\n",
    "        log_prob = policy.log_prob(action).sum(-1, keepdim=True)\n",
    "        return action, log_prob\n",
    "\n",
    "    def act(self, state: np.ndarray, device: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Determines the action to take for a given state.\n",
    "\n",
    "        Parameters:\n",
    "        - state (np.ndarray): The state array.\n",
    "        - device (str): The device to perform computations on.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray: The action array.\n",
    "\n",
    "        Description:\n",
    "        - Converts the state to a PyTorch tensor and sends it to the specified device.\n",
    "        - Computes the policy distribution for the given state.\n",
    "        - If the network is in training mode, samples an action from the distribution; otherwise, uses the mean of the distribution.\n",
    "        - Converts the action back to a NumPy array and returns it.\n",
    "        \"\"\"\n",
    "        state_t = torch.tensor(state[None], dtype=torch.float32, device=device)\n",
    "        policy = self._get_policy(state_t)\n",
    "        if self._mlp.training:\n",
    "            action_t = policy.sample()\n",
    "        else:\n",
    "            action_t = policy.mean\n",
    "        action = action_t[0].cpu().numpy()\n",
    "        return action\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be95f25d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d85b36533b6d978b46529876e0e3b4fe",
     "grade": false,
     "grade_id": "cell-54a35d69eb17ab6c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Deep Q-Network (Critic)\n",
    "\n",
    "The critic network is trained to represent the Q-function, the input is the state `s` and action `a`, and the output is Q value of this state-action pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c600ed",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3f2f763df7bcb685d060aeab18cb94c",
     "grade": false,
     "grade_id": "cell-9adf2146db5c1911",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        hidden_dim: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the Critic model, which is a neural network used for value estimation in reinforcement learning.\n",
    "\n",
    "        The Critic model evaluates the quality of actions taken in a given state. It is used to guide the training\n",
    "        of the Actor model by providing feedback on the expected returns of the actions it selects.\n",
    "\n",
    "        Parameters:\n",
    "        - state_dim (int): The dimensionality of the state space.\n",
    "        - action_dim (int): The dimensionality of the action space.\n",
    "        - hidden_dim (int): The number of neurons in each hidden layer.\n",
    "\n",
    "        The Critic network takes both state and action as inputs and outputs a Q-value representing the expected return\n",
    "        of taking the given action in the given state. The network consists of a Multi-Layer Perceptron (MLP) with ReLU\n",
    "        activations.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._mlp = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the forward pass through the Critic network.\n",
    "\n",
    "        Parameters:\n",
    "        - state (torch.Tensor): The state tensor for which the Q-value is to be evaluated.\n",
    "        - action (torch.Tensor): The action tensor for which the Q-value is to be evaluated.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The Q-value of the given state-action pair.\n",
    "\n",
    "        This method concatenates the state and action tensors and passes them through the MLP to output\n",
    "        a Q-value. This value represents the Critic's estimate of the expected return for taking the given\n",
    "        action in the given state.\n",
    "        \"\"\"\n",
    "        q_value = self._mlp(torch.cat([state, action], dim=-1))\n",
    "        return q_value\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c9a07c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d978e37bc1ad282efe8ec390662b2d89",
     "grade": false,
     "grade_id": "cell-527bf19af2108055",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Replay Buffer\n",
    "\n",
    "The Replay Buffer stores transitions (state, action, reward, next state, done). These stored transitions are used to train the agent by sampling random mini-batches, enabling the agent to learn from past experiences (replay). Different from the online replay buffer that used in online RL, in this exercise, we will use a fixed offline replay buffer that loaded from an offline collected dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50e84fc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd8f92da2f6e763fb8ed0316519b68fb",
     "grade": false,
     "grade_id": "cell-2a7b70762628293f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        buffer_size: int,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a Replay Buffer for storing experiences in reinforcement learning.\n",
    "\n",
    "        Parameters:\n",
    "        - state_dim (int): Dimensionality of the state space.\n",
    "        - action_dim (int): Dimensionality of the action space.\n",
    "        - buffer_size (int): The maximum number of transitions the buffer can hold.\n",
    "        - device (str): The device on which the tensors are stored. Default is 'cpu'.\n",
    "\n",
    "        The buffer initializes tensors to store states, actions, rewards, next states, and done flags.\n",
    "        Each tensor has a size determined by the buffer_size and the respective dimensionality of its content.\n",
    "        \"\"\"\n",
    "        self._buffer_size = buffer_size\n",
    "        self._pointer = 0\n",
    "        self._size = 0\n",
    "\n",
    "        self._states = torch.zeros(\n",
    "            (buffer_size, state_dim), dtype=torch.float32, device=device\n",
    "        )\n",
    "        self._actions = torch.zeros(\n",
    "            (buffer_size, action_dim), dtype=torch.float32, device=device\n",
    "        )\n",
    "        self._rewards = torch.zeros((buffer_size, 1), dtype=torch.float32, device=device)\n",
    "        self._next_states = torch.zeros(\n",
    "            (buffer_size, state_dim), dtype=torch.float32, device=device\n",
    "        )\n",
    "        self._dones = torch.zeros((buffer_size, 1), dtype=torch.float32, device=device)\n",
    "        self._device = device\n",
    "\n",
    "    def _to_tensor(self, data: np.ndarray) -> torch.Tensor:\n",
    "        return torch.as_tensor(data, dtype=torch.float32, device=self._device)\n",
    "    \n",
    "    def _check_empty_and_fit(self, n: int) -> None:\n",
    "        if self._size != 0:\n",
    "            raise ValueError(\"Trying to load data into a non-empty replay buffer.\")\n",
    "        if n > self._buffer_size:\n",
    "            raise ValueError(f\"Replay buffer ({self._buffer_size}) is smaller than dataset ({n}).\")\n",
    "\n",
    "\n",
    "    def load_minari_dataset(self, minari_dataset) -> None:\n",
    "        \"\"\"\n",
    "        Load a Minari dataset (episode format) into the replay buffer.\n",
    "        Flattens episodes into transitions.\n",
    "\n",
    "        Expects each episode to have:\n",
    "          observations: (T+1, obs_dim)\n",
    "          actions:      (T, act_dim)\n",
    "          rewards:      (T,)\n",
    "          terminations: (T,)\n",
    "          truncations:  (T,)\n",
    "        \"\"\"\n",
    "        # First pass: count transitions to allocate into buffer cleanly\n",
    "        n_total = 0\n",
    "        for ep in minari_dataset.iterate_episodes():\n",
    "            n_total += len(ep.actions)\n",
    "\n",
    "        self._check_empty_and_fit(n_total)\n",
    "\n",
    "        # Second pass: write into preallocated tensors\n",
    "        idx = 0\n",
    "        for ep in minari_dataset.iterate_episodes():\n",
    "            obs = np.asarray(ep.observations)\n",
    "            act = np.asarray(ep.actions)\n",
    "            rew = np.asarray(ep.rewards).reshape(-1)\n",
    "            term = np.asarray(ep.terminations).astype(np.float32).reshape(-1)\n",
    "            trunc = np.asarray(ep.truncations).astype(np.float32).reshape(-1)\n",
    "\n",
    "            T = act.shape[0]\n",
    "            obs_t = obs[:-1]\n",
    "            next_obs_t = obs[1:]\n",
    "            done = np.clip(term + trunc, 0.0, 1.0)  # terminated OR truncated\n",
    "\n",
    "            sl = slice(idx, idx + T)\n",
    "            self._states[sl] = self._to_tensor(obs_t)\n",
    "            self._actions[sl] = self._to_tensor(act)\n",
    "            self._rewards[sl] = self._to_tensor(rew.reshape(-1, 1))\n",
    "            self._next_states[sl] = self._to_tensor(next_obs_t)\n",
    "            self._dones[sl] = self._to_tensor(done.reshape(-1, 1))\n",
    "\n",
    "            idx += T\n",
    "\n",
    "        self._size = n_total\n",
    "        self._pointer = n_total\n",
    "        print(f\"Dataset size: {n_total}\")\n",
    "\n",
    "    def sample(self, batch_size: int) -> List[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Samples a mini-batch of experiences from the buffer.\n",
    "\n",
    "        Parameters:\n",
    "        - batch_size (int): The size of the mini-batch to sample.\n",
    "\n",
    "        Returns:\n",
    "        - List[torch.Tensor]: A list containing tensors of states, actions, rewards, next states, and dones.\n",
    "\n",
    "        Randomly samples a batch of experiences from the buffer. This method is used during the training\n",
    "        process to provide the agent with a diverse set of experiences from which to learn, breaking the\n",
    "        correlation present in sequential experiences.\n",
    "        \"\"\"\n",
    "        indices = np.random.randint(0, self._size, size=batch_size)\n",
    "        states = self._states[indices]\n",
    "        actions = self._actions[indices]\n",
    "        rewards = self._rewards[indices]\n",
    "        next_states = self._next_states[indices]\n",
    "        dones = self._dones[indices]\n",
    "        return [states, actions, rewards, next_states, dones]\n",
    "\n",
    "    def normalize_states(\n",
    "        self,\n",
    "        state_mean: Union[np.ndarray, float],\n",
    "        state_std: Union[np.ndarray, float],\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Normalize the states and next_states in the buffer using the provided mean and std.\n",
    "\n",
    "        Parameters:\n",
    "        - state_mean: The mean used for normalization.\n",
    "        - state_std: The standard deviation used for normalization.\n",
    "        \"\"\"\n",
    "        state_mean_t = torch.as_tensor(state_mean, dtype=torch.float32, device=self._device)\n",
    "        state_std_t = torch.as_tensor(state_std, dtype=torch.float32, device=self._device)\n",
    "\n",
    "        self._states[:self._size] = (self._states[:self._size] - state_mean_t) / state_std_t\n",
    "        self._next_states[:self._size] = (self._next_states[:self._size] - state_mean_t) / state_std_t\n",
    "        print(\"States normalized.\")\n",
    "\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16643a6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b569956faf2a5e2342677959419fe80a",
     "grade": false,
     "grade_id": "cell-965b592649bb01ac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Setup Environment Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76e7452",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c9a4fef56ce69dc4b662b97c69bb22f7",
     "grade": false,
     "grade_id": "cell-5a992b377a155081",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "DATASET_ID = \"mujoco/invertedpendulum/medium-v0\"\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LEARNING_RATE = 3e-4\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d36ad11",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "39fccb9c8e03fbf0c4147d6ea822fe6e",
     "grade": false,
     "grade_id": "cell-1ade0ce9c6d3e8e6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 2: Behavior Cloning\n",
    "**Behavioral Cloning (BC)** is a classic imitation learning method where a model, typically a neural network, is trained to \"cloning\" the behavioral of an expert from expert demonstrations.\n",
    "\n",
    "1. **Data Collection:**\n",
    "   - Collect a dataset $\\mathcal{D}$ of state-action pairs $\\{(s_i, a_i)\\}$ from expert demonstrations.\n",
    "\n",
    "2. **Initialize:**\n",
    "   - Initialize the policy network $\\pi_\\theta(a|s)$ with parameters $\\theta$.\n",
    "\n",
    "3. **Training Loop:**\n",
    "   - **For each epoch** or until convergence:\n",
    "     - Shuffle the dataset $\\mathcal{D}$.\n",
    "     - **For each mini-batch** $\\mathcal{B}$ in $\\mathcal{D}$:\n",
    "       - Extract mini-batch of state-action pairs $(s, a)$.\n",
    "       - Compute the log-likelihood loss: $$ L(\\theta) = -\\frac{1}{|\\mathcal{B}|} \\sum_{(s, a) \\in B} \\log \\pi_\\theta(a|s) $$\n",
    "       - Update the parameters $\\theta$ of the policy network using gradient descent to minimize the loss.\n",
    "\n",
    "Tip:\n",
    "- There are multiple choices of the loss function in behavioral cloning, e.g., Mean Square Error (MSE) and Cross Entropy (CE). In this exercise, we focus on maximizing the log-likelihood as it is more straightforward to compare with AWAC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5479e0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8292a0ad7822c33b9c4d0b860f676c74",
     "grade": false,
     "grade_id": "cell-76d267a6864f68dd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class BehavioralCloning:\n",
    "    \"\"\"\n",
    "    A Behavioral Cloning class for training an agent using expert demonstrations.\n",
    "\n",
    "    Attributes:\n",
    "        _actor (nn.Module): The policy network that outputs actions given states.\n",
    "        _actor_optimizer (torch.optim.Optimizer): Optimizer for training the actor.\n",
    "        _device (str): The device (e.g., 'cpu' or 'cuda') on which to perform computations.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 actor: nn.Module,\n",
    "                 actor_optimizer: torch.optim.Optimizer,\n",
    "                 device: str = \"cpu\"):\n",
    "        \"\"\"\n",
    "        Initializes the BehavioralCloning class with an actor network, its optimizer, and the device.\n",
    "\n",
    "        Args:\n",
    "            actor (nn.Module): The policy network.\n",
    "            actor_optimizer (torch.optim.Optimizer): Optimizer for the actor.\n",
    "            device (str, optional): Computation device, 'cpu' by default.\n",
    "        \"\"\"\n",
    "        self._actor = actor\n",
    "        self._actor_optimizer = actor_optimizer\n",
    "        self._device = device\n",
    "\n",
    "    def _actor_loss(self, states: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the loss for the actor based on the log likelihood of the actions given the states.\n",
    "\n",
    "        Args:\n",
    "            states (torch.Tensor): The input states.\n",
    "            actions (torch.Tensor): The expert actions corresponding to the states.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The computed loss for the actor.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return loss\n",
    "\n",
    "    def _update_actor(self, states: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs a single optimization step for the actor.\n",
    "\n",
    "        Args:\n",
    "            states (torch.Tensor): States from the replay buffer.\n",
    "            actions (torch.Tensor): Corresponding actions from the replay buffer.\n",
    "\n",
    "        Returns:\n",
    "            float: The loss value after the optimization step.\n",
    "        \"\"\"\n",
    "        self._actor_optimizer.zero_grad()\n",
    "        loss = self._actor_loss(states, actions)\n",
    "        loss.backward()\n",
    "        self._actor_optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def update(self,\n",
    "              replay_buffer: ReplayBuffer,\n",
    "              batch_size: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Updates the actor using a batch of data from the replay buffer.\n",
    "\n",
    "        Args:\n",
    "            replay_buffer (ReplayBuffer): The replay buffer to sample experiences from.\n",
    "            batch_size (int): The number of samples to draw from the replay buffer.\n",
    "\n",
    "        Returns:\n",
    "            float: The loss value from the actor update.\n",
    "        \"\"\"\n",
    "        states, actions, _, _, _ = replay_buffer.sample(batch_size)\n",
    "        actor_loss = self._update_actor(states, actions)\n",
    "        return actor_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb624def",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "70250c586ff23a8840cbb4722121387b",
     "grade": true,
     "grade_id": "cell-8b5a9eed4dc8caa6",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# ID: cell-8b5a9eed4dc8caa6 - possible points: 3\n",
    "\n",
    "# Test BehavioralCloning implementation\n",
    "print(\"Testing BehavioralCloning...\")\n",
    "\n",
    "# Create test actor and BC instance\n",
    "_test_actor = Actor(state_dim=4, action_dim=1, hidden_dim=64)\n",
    "_test_actor.to(\"cpu\")\n",
    "_test_bc = BehavioralCloning(\n",
    "    actor=_test_actor,\n",
    "    actor_optimizer=torch.optim.Adam(_test_actor.parameters(), lr=3e-4),\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "_test_states = torch.randn(32, 4)\n",
    "_test_actions = torch.randn(32, 1).clamp(-1, 1)\n",
    "_test_loss = _test_bc._actor_loss(_test_states, _test_actions)\n",
    "\n",
    "assert _test_loss.ndim == 0, \"Loss should be a scalar\"\n",
    "assert not torch.isnan(_test_loss), \"Loss should not be NaN\"\n",
    "\n",
    "print(\"✓ BehavioralCloning passed basic tests!\")\n",
    "\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eaf883",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "977e32300a7a274583f3778f20f427c9",
     "grade": false,
     "grade_id": "cell-4318d4182a5681d7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "def bc_train():\n",
    "    # Load dataset and environment\n",
    "    minari_dataset = minari.load_dataset(DATASET_ID, download=True)\n",
    "    env = minari_dataset.recover_environment(eval_env=True)\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "\n",
    "    # Compute normalization statistics from the dataset\n",
    "    state_mean, state_std, _, _ = compute_mean_std_from_minari(minari_dataset)\n",
    "\n",
    "    # Wrap the environment to apply state normalization during evaluation\n",
    "    env = wrap_env(env, state_mean=state_mean, state_std=state_std)\n",
    "\n",
    "    replay_buffer = ReplayBuffer(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        buffer_size=2_000_000,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "\n",
    "    replay_buffer.load_minari_dataset(minari_dataset)\n",
    "    # Normalize observations in the replay buffer\n",
    "    replay_buffer.normalize_states(state_mean, state_std)\n",
    "    set_seed(SEED, env)\n",
    "\n",
    "    kwargs = {\n",
    "        \"state_dim\": state_dim,\n",
    "        \"action_dim\": action_dim,\n",
    "        \"hidden_dim\": 256,\n",
    "    }\n",
    "\n",
    "    actor = Actor(min_action=float(env.action_space.low[0]), max_action=float(env.action_space.high[0]), **kwargs)\n",
    "    actor.to(DEVICE)\n",
    "    actor_optimizer = torch.optim.Adam(actor.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    bc = BehavioralCloning(actor=actor,\n",
    "                           actor_optimizer=actor_optimizer,\n",
    "                           device=DEVICE)\n",
    "\n",
    "    eval_score_list = [ ]\n",
    "    iteration_list = [ ]\n",
    "    for t in trange(20000, ncols=100):\n",
    "        actor_loss = bc.update(replay_buffer, batch_size=256)\n",
    "        if (t + 1) % 500 == 0:\n",
    "            eval_scores = evaluate_policy(env, actor, num_episodes=30)\n",
    "            print(f\"Eval_scores: {eval_scores.mean():.2f} +/- {eval_scores.std():.2f}\")\n",
    "            print(f\"Actor loss: {actor_loss:.4f}\")\n",
    "            eval_score_list.append(eval_scores.mean())\n",
    "            iteration_list.append(t+1)\n",
    "            update_plot(iteration_list, eval_score_list, 'BC')\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025f20b6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fe8041e7aa683da09aae1a5bac88e4b1",
     "grade": false,
     "grade_id": "cell-787f72ef9b08442c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Run the following block to train your BC agent!\n",
    "\n",
    "Hint:\n",
    "- On the InvertedPendulum environment, you should see episode returns improving towards 130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f22141",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f373f24e172427793ec4434533625b20",
     "grade": false,
     "grade_id": "cell-47d3dda108ede6fd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "bc_train()\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500b323f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "61aa82f033ab7ce19b961129e425c538",
     "grade": false,
     "grade_id": "cell-cc805936bd4a76cf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 3: Advantage-Weighted Actor-Critic (AWAC) (8pts)\n",
    "In this part, we will implement the AWAC algorithm with double critic, which is similar to the technique used in Soft Actor-Critic(SAC) and TD3. The algorithm looks as follow. The AWAC algorithm can be used both as online or offline algorithm, in this exercise we focus only on its offline variant.\n",
    "\n",
    "1. **Initialize:**\n",
    "   - Initialize actor network $\\pi_\\theta(a|s)$ with parameters $\\theta$.\n",
    "   - Initialize two critic networks $Q_{\\phi_1}(s, a)$ and $Q_{\\phi_2}(s, a)$ with parameters $\\phi_1$ and $\\phi_2$.\n",
    "   - Initialize target critic networks $Q_{\\phi'_1}$ and $Q_{\\phi'_2}$ with parameters $\\phi'_1 \\leftarrow \\phi_1$ and $\\phi'_2 \\leftarrow \\phi_2$.\n",
    "   - Initialize replay buffer $\\mathcal{D}$.\n",
    "\n",
    "2. **For each iteration:**\n",
    "   - **For each gradient step:**\n",
    "     - Sample a batch of transitions $(s, a, r, s', d)$ from $\\mathcal{D}$.\n",
    "     - Compute target values using the smaller Q value of the two target critics: $$ y = r + \\gamma (1 - d) \\min_{i=1,2} Q_{\\phi'_i}(s', a') $$\n",
    "     - Update each critic by minimizing the loss: $$ L(\\phi_i) = \\frac{1}{|B|} \\sum_{(s, a, r, s', d) \\in B} (Q_{\\phi_i}(s, a) - y)^2 \\quad \\text{for } i=1,2 $$\n",
    "     - Compute the advantage for a given state action pair (s, a):\n",
    "     \\begin{equation}\n",
    "     A(s,a)=\\min_{i=1,2}Q_{\\phi_i}(s,a) - \\mathbb{E}_{a^\\pi \\sim \\pi_{\\theta}(a|s)}[\\min_{i=1,2}Q_{\\phi_i}(s,a^\\pi)]\n",
    "     \\end{equation}\n",
    "     \n",
    "     **Important Hint** : here we use the expectation of Q function to represent the $V(s)$. In theory, we need multiple samples to get better estimation, however, only **one sample** usually works fine in practice.\n",
    "     - Compute advantage-weighted policy gradient: $$ \\nabla_\\theta J(\\theta) = \\frac{1}{|B|} \\sum_{(s, a) \\in B} \\nabla_\\theta \\log \\pi_\\theta(a|s) \\exp \\left( \\frac{1}{\\lambda} A(s,a) \\right)$$\n",
    "     - Update actor network by gradient ascent: $$ \\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta) $$\n",
    "     - Softly update target critic networks: $$ \\phi'_i \\leftarrow \\tau \\phi_i + (1 - \\tau) \\phi'_i \\quad \\text{for } i=1,2 $$\n",
    "\n",
    "3. **Repeat** until convergence or maximum number of iterations reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443452a5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53f94a99b749ec650ab90ce4f66c9ca4",
     "grade": false,
     "grade_id": "cell-bcc8726c870cbe4e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class AdvantageWeightActorCritic:\n",
    "    def __init__(\n",
    "        self,\n",
    "        actor: nn.Module,\n",
    "        actor_optimizer: torch.optim.Optimizer,\n",
    "        critic_1: nn.Module,\n",
    "        critic_1_optimizer: torch.optim.Optimizer,\n",
    "        critic_2: nn.Module,\n",
    "        critic_2_optimizer: torch.optim.Optimizer,\n",
    "        gamma: float = 0.99,\n",
    "        tau: float = 0.005,\n",
    "        awac_lambda: float = 1.0,\n",
    "        exp_adv_max: float = 100.0,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the Advantage Weighted Actor-Critic (AWAC) agent.\n",
    "\n",
    "        AWAC combines off-policy actor-critic methods with an advantage-weighted behavioral cloning loss to stabilize\n",
    "        and improve the training of deep reinforcement learning policies.\n",
    "\n",
    "        Parameters:\n",
    "        - actor (nn.Module): The actor network.\n",
    "        - actor_optimizer (torch.optim.Optimizer): Optimizer for the actor network.\n",
    "        - critic_1 (nn.Module): The first critic network.\n",
    "        - critic_1_optimizer (torch.optim.Optimizer): Optimizer for the first critic network.\n",
    "        - critic_2 (nn.Module): The second critic network.\n",
    "        - critic_2_optimizer (torch.optim.Optimizer): Optimizer for the second critic network.\n",
    "        - gamma (float): Discount factor for future rewards. Default is 0.99.\n",
    "        - tau (float): Soft update rate for the target networks. Default is 0.005.\n",
    "        - awac_lambda (float): Scaling factor for advantage weights in the actor loss. Default is 1.0.\n",
    "        - exp_adv_max (float): Maximum limit for exponentiated advantages. Default is 100.0.\n",
    "        - device (str): The device on which to perform computations. Default is 'cpu'.\n",
    "\n",
    "        This implementation uses two critic networks to estimate the value function, along with their target networks,\n",
    "        which are updated using a soft update strategy. The actor's policy is updated using an advantage-weighted\n",
    "        loss to encourage actions that lead to higher returns than currently estimated.\n",
    "        \"\"\"\n",
    "        self._actor = actor\n",
    "        self._actor_optimizer = actor_optimizer\n",
    "\n",
    "        self._critic_1 = critic_1\n",
    "        self._critic_1_optimizer = critic_1_optimizer\n",
    "        self._target_critic_1 = deepcopy(critic_1)\n",
    "\n",
    "        self._critic_2 = critic_2\n",
    "        self._critic_2_optimizer = critic_2_optimizer\n",
    "        self._target_critic_2 = deepcopy(critic_2)\n",
    "\n",
    "        self._gamma = gamma\n",
    "        self._tau = tau\n",
    "        self._awac_lambda = awac_lambda\n",
    "        self._exp_adv_max = exp_adv_max\n",
    "\n",
    "        self._device = device\n",
    "\n",
    "    def _actor_loss(self, states: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the loss for the actor network.\n",
    "\n",
    "        The loss is calculated using an advantage-weighted behavioral cloning approach. It encourages the actor to\n",
    "        take actions that lead to higher returns compared to the current policy's estimate.\n",
    "\n",
    "        Parameters:\n",
    "        - states (torch.Tensor): The batch of states.\n",
    "        - actions (torch.Tensor): The batch of actions.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The computed loss for the actor network.\n",
    "\n",
    "        The advantage is calculated as the difference between the Q-values from the critics for the actual\n",
    "        actions and the estimated V-values from the critics for the actions sampled by the actor. These advantages\n",
    "        are then scaled and used to weight the log probabilities of the actions in the actor's policy.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            pi_actions, _ = self._actor(states)\n",
    "\n",
    "            \"\"\"\n",
    "            Calculate advantages and the weights\n",
    "              Hints:\n",
    "                1. V(s) can be represented using expectation of Q(s,a).\n",
    "                   In practice, one sample could be enough\n",
    "                2. self._actor() return action samples from the actor\n",
    "                3. Weights should not contain gradient\n",
    "            \"\"\"\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "\n",
    "            weights = torch.clamp(weights, max=self._exp_adv_max)\n",
    "\n",
    "        ### Implement the AWAC loss ###\n",
    "        \"\"\"\n",
    "        Hint: be careful about the sign of loss, we are trying to maximize the\n",
    "        objective.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return loss\n",
    "\n",
    "    def _critic_loss(self, states, actions, rewards, next_states, dones):\n",
    "        \"\"\"\n",
    "        Computes the loss for the critic networks.\n",
    "\n",
    "        The loss is the Mean Squared Error (MSE) between the critics' Q-value estimates and the target Q-values,\n",
    "        which are computed using the Bellman equation.\n",
    "\n",
    "        Parameters:\n",
    "        - states, actions, rewards, next_states, dones: Tensors representing the batch of transitions.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The computed loss for the critic networks.\n",
    "\n",
    "        The target Q-values are calculated using the minimum Q-value of the two target critics for the next state\n",
    "        and action pair, adjusted by the reward and discount factor.\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        Hints:\n",
    "          1. next_action could be compute by calling self._actor(next_states)\n",
    "          2. targets should not contain gradient\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        loss = critic_1_loss + critic_2_loss\n",
    "        return loss\n",
    "\n",
    "    def _update_target_network(self, target_network: nn.Module, network: nn.Module):\n",
    "        \"\"\"\n",
    "        Performs a soft update on the target network.\n",
    "\n",
    "        Parameters:\n",
    "        - target_network (nn.Module): The target network to be updated.\n",
    "        - network (nn.Module): The current network whose parameters are used for the update.\n",
    "\n",
    "        This method updates the target network's parameters by blending them with the parameters from the\n",
    "        current network, controlled by the tau parameter. This soft update helps in stabilizing training by\n",
    "        providing a slowly changing target for the critics.\n",
    "        \"\"\"\n",
    "        for target_param, param in zip(target_network.parameters(), network.parameters()):\n",
    "            target_param.data.copy_(self._tau * param.data + (1.0 - self._tau) * target_param.data)\n",
    "\n",
    "    def _update_actor(self, states: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Updates the actor network by minimizing its loss.\n",
    "\n",
    "        Parameters:\n",
    "        - states (torch.Tensor): The batch of states.\n",
    "        - actions (torch.Tensor): The batch of actions.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The actor loss after the update.\n",
    "\n",
    "        This method performs a gradient descent step to update the actor's parameters in order to minimize\n",
    "        the advantage-weighted loss.\n",
    "        \"\"\"\n",
    "        self._actor_optimizer.zero_grad()\n",
    "        loss = self._actor_loss(states, actions)\n",
    "        loss.backward()\n",
    "        self._actor_optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def _update_critics(\n",
    "        self,\n",
    "        states: torch.Tensor,\n",
    "        actions: torch.Tensor,\n",
    "        rewards: torch.Tensor,\n",
    "        next_states: torch.Tensor,\n",
    "        dones: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Updates the critic networks by minimizing their loss.\n",
    "\n",
    "        Parameters:\n",
    "        - states, actions, rewards, next_states, dones: Tensors representing the batch of transitions.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The critic loss after the update.\n",
    "\n",
    "        This method performs a gradient descent step to update the critic networks' parameters in order to minimize\n",
    "        the MSE loss between their Q-value estimates and the target Q-values.\n",
    "        \"\"\"\n",
    "        self._critic_1_optimizer.zero_grad()\n",
    "        self._critic_2_optimizer.zero_grad()\n",
    "        loss = self._critic_loss(states, actions, rewards, next_states, dones)\n",
    "        loss.backward()\n",
    "        self._critic_1_optimizer.step()\n",
    "        self._critic_2_optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        replay_buffer: ReplayBuffer,\n",
    "        batch_size: int,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Performs a training update on both the actor and critic networks.\n",
    "\n",
    "        Parameters:\n",
    "        - replay_buffer (ReplayBuffer): The replay buffer to sample transitions from.\n",
    "        - batch_size (int): The number of transitions to sample for the update.\n",
    "\n",
    "        Returns:\n",
    "        - Tuple[torch.Tensor, torch.Tensor]: The actor and critic losses after the update.\n",
    "\n",
    "        This method samples a batch of transitions from the replay buffer and uses them to update both the actor\n",
    "        and critic networks. It also updates the target critic networks using the soft update method.\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "        critic_loss = self._update_critics(states, actions, rewards, next_states, dones)\n",
    "        actor_loss = self._update_actor(states, actions)\n",
    "        self._update_target_network(self._target_critic_1, self._critic_1)\n",
    "        self._update_target_network(self._target_critic_2, self._critic_2)\n",
    "        return actor_loss, critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571b6991",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "add36e847b96a59f30fde0771302f459",
     "grade": true,
     "grade_id": "cell-9fcbd0bfc147033b",
     "locked": true,
     "points": 8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# ID: cell-9fcbd0bfc147033b - possible points: 8\n",
    "\n",
    "# Test AWAC implementation\n",
    "print(\"Testing AWAC...\")\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "_test_awac_actor = Actor(state_dim=4, action_dim=1, hidden_dim=64)\n",
    "_test_awac_critic1 = Critic(state_dim=4, action_dim=1, hidden_dim=64)\n",
    "_test_awac_critic2 = Critic(state_dim=4, action_dim=1, hidden_dim=64)\n",
    "\n",
    "_test_awac = AdvantageWeightActorCritic(\n",
    "    actor=_test_awac_actor,\n",
    "    actor_optimizer=torch.optim.Adam(_test_awac_actor.parameters(), lr=3e-4),\n",
    "    critic_1=_test_awac_critic1,\n",
    "    critic_1_optimizer=torch.optim.Adam(_test_awac_critic1.parameters(), lr=3e-4),\n",
    "    critic_2=_test_awac_critic2,\n",
    "    critic_2_optimizer=torch.optim.Adam(_test_awac_critic2.parameters(), lr=3e-4),\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "_test_states = torch.randn(32, 4)\n",
    "_test_actions = torch.randn(32, 1).clamp(-1, 1)\n",
    "_test_rewards = torch.randn(32, 1)\n",
    "_test_next_states = torch.randn(32, 4)\n",
    "_test_dones = torch.zeros(32, 1)\n",
    "\n",
    "# Test actor loss\n",
    "_actor_loss = _test_awac._actor_loss(_test_states, _test_actions)\n",
    "assert _actor_loss.ndim == 0, \"Actor loss should be a scalar\"\n",
    "assert not torch.isnan(_actor_loss), \"Actor loss should not be NaN\"\n",
    "\n",
    "# Test critic loss\n",
    "_critic_loss = _test_awac._critic_loss(_test_states, _test_actions, _test_rewards, _test_next_states, _test_dones)\n",
    "assert _critic_loss.ndim == 0, \"Critic loss should be a scalar\"\n",
    "assert _critic_loss.item() >= 0, \"MSE loss should be non-negative\"\n",
    "\n",
    "print(\"✓ AWAC passed basic tests!\")\n",
    "\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7901da61",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a88e3a4cc6ccf7ca6ee16779f105bf19",
     "grade": false,
     "grade_id": "cell-caa8076c18a5fe5e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "def awac_train():\n",
    "    # Load dataset and environment\n",
    "    minari_dataset = minari.load_dataset(DATASET_ID, download=True)\n",
    "    env = minari_dataset.recover_environment(eval_env=True)\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "\n",
    "    # Compute normalization statistics from the dataset\n",
    "    state_mean, state_std, _, _ = compute_mean_std_from_minari(minari_dataset)\n",
    "\n",
    "    # Wrap the environment to apply state normalization during evaluation\n",
    "    env = wrap_env(env, state_mean=state_mean, state_std=state_std)\n",
    "\n",
    "    # Initialize replay buffer\n",
    "    replay_buffer = ReplayBuffer(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        buffer_size=2_000_000,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    replay_buffer.load_minari_dataset(minari_dataset)\n",
    "    # Normalize observations in the replay buffer\n",
    "    replay_buffer.normalize_states(state_mean, state_std)\n",
    "    set_seed(SEED, env)\n",
    "\n",
    "    kwargs = {\n",
    "        \"state_dim\": state_dim,\n",
    "        \"action_dim\": action_dim,\n",
    "        \"hidden_dim\": 256,\n",
    "    }\n",
    "\n",
    "    actor = Actor(min_action=float(env.action_space.low[0]), max_action=float(env.action_space.high[0]), **kwargs)\n",
    "    actor.to(DEVICE)\n",
    "    actor_optimizer = torch.optim.Adam(actor.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    critic_1 = Critic(**kwargs)\n",
    "    critic_1.to(DEVICE)\n",
    "    critic_1_optimizer = torch.optim.Adam(critic_1.parameters(), lr=LEARNING_RATE)\n",
    "    critic_2 = Critic(**kwargs)\n",
    "    critic_2.to(DEVICE)\n",
    "    critic_2_optimizer = torch.optim.Adam(critic_2.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    awac = AdvantageWeightActorCritic(\n",
    "        actor=actor,\n",
    "        actor_optimizer=actor_optimizer,\n",
    "        critic_1=critic_1,\n",
    "        critic_1_optimizer=critic_1_optimizer,\n",
    "        critic_2=critic_2,\n",
    "        critic_2_optimizer=critic_2_optimizer,\n",
    "        gamma=0.99,\n",
    "        tau=0.005,\n",
    "        awac_lambda=1.0,\n",
    "        exp_adv_max=100.0,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "\n",
    "    eval_score_list = []\n",
    "    iteration_list = []\n",
    "    for t in trange(20000, ncols=100):\n",
    "        actor_loss, critic_loss = awac.update(replay_buffer, batch_size=256)\n",
    "        if (t + 1) % 500 == 0:\n",
    "            eval_scores = evaluate_policy(env, actor, num_episodes=30)\n",
    "            print(f\"Eval_scores: {eval_scores.mean():.2f} +/- {eval_scores.std():.2f}\")\n",
    "            print(f\"Actor loss: {actor_loss:.4f}, critic loss: {critic_loss:.4f}\")\n",
    "            eval_score_list.append(eval_scores.mean())\n",
    "            iteration_list.append(t+1)\n",
    "            update_plot(iteration_list, eval_score_list, 'AWAC')\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddeec01",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3311103ca032e371b72fb54baed35f8b",
     "grade": false,
     "grade_id": "cell-1e47e1d2897a2db6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Run the following block to train the AWAC agent!\n",
    "\n",
    "Hints:\n",
    "- The AWAC agent should achieve better performance than BC\n",
    "- On the InvertedPendulum environment, AWAC should achieve episode returns close to 1000\n",
    "- There may be some fluctuation at the beginning of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d3e7ba",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61dacc87b1a7108d643c95774bf6c0d5",
     "grade": false,
     "grade_id": "cell-54b2bd1fcc669e1e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "awac_train()\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d461115",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b7beb6458c1114c60feb53e1353ae12",
     "grade": false,
     "grade_id": "cell-ea9d6e991cff8a63",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Self-test questions (No Points)\n",
    "1. Why AWAC can achieve better performance than BC?\n",
    "\n",
    "\n",
    "2. Why AWAC uses policy gradient instead of reparameterization gradient?\n",
    "\n",
    "\n",
    "3. What are the challenges of applying offline RL in the real world? (Open-ended)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
